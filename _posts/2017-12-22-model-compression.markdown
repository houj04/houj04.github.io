---
layout: post
title: "模型压缩综述"
categories: mypost
---

[TOC]

## 模型压缩综述

神经网络（artificial neural network）在各种应用中大显身手，不过随着移动端设备的发展和广泛应用，在计算资源（计算能力、内存、电力）有限的各种移动设备上进行神经网络的推理（inference）的需求逐渐增加，因此产生了“压缩神经网络模型”的需求。另一方面，训练（train）一个神经网络同样也需要大量的算力和空间，因此催生了在推理和训练两方面对神经网络进行“压缩”的需求。

“压缩”的本质是去除冗余。对于一个训练好的神经网络来说，通常它包括两种类型的层（layer），一种是卷积层（convolution layer），另一种是全连接层（fully-connected layer）。在一个典型的神经网络中，卷积层通常消耗了最多的计算资源，而全连接层通常消耗了最多的存储资源。因此有了如下几个大类的从不同方面进行压缩的方法。

## 剪枝

在通常的全连接层（fully-connected layer）中，参数表现为一个矩阵的形式。矩阵中的参数有一些绝对值比较大，有一些绝对值比较小。最直观的剪枝（pruning）策略是将权重值小于某一个固定阈值的权重设置为0，也即将某个输入神经元到某个输出神经元的连接“去掉”。经过剪枝处理之后的全连接层不再是全连接，而是稀疏（sparse）连接。

## 量化

典型的神经网络在训练时候使用的都是32比特（bit）单精度浮点数（single precision floating point）。众所周知，浮点数的标准中有一部分用来保证精度，有一部分用来保证范围。在某个固定的训练好的模型中，权重的范围并不会很大，不需要那么多的位数来保存范围。另一方面，经过测试发现，权重的精度也未必需要那么多位数来保存，因此可以通过量化（quantization）的方法将单精度浮点数降低到16比特的半精度浮点数，或者更低。也有一些方法直接映射到整数进行计算，整数计算和同等长度的浮点计算相比，速度会更快。

## 权值共享

针对某个矩阵中的全部元素，可以通过类似k均值聚类（k-means）的方法分为若干个类（cluster），权重值接近的若干连接会处于同一个类之中。然后将同一个类中的权重用某一个固定值代替，这样矩阵中的元素就转换成为了某个查找表中的“下标”（index）。和“量化”有些不同的是，在“量化”中，权重被降低到某个固定的精度数值，不同的权重之间没有互相影响的关联性。而在“权值共享”中，一个矩阵中的若干个权重共享同一个高精度的浮点数，比特数能降低到多少取决于聚类的个数，或者分桶的桶（bucket）数。

## 分解

以全连接层为例，一批输入“通过”某个全连接层，其实是输入矩阵（假设输入是一小批数据（mini-batch））乘以该层的参数矩阵。如果能够将参数矩阵自身进行压缩，就能够显著减少存储量。例如，将一个n×m的矩阵，分解成n×p和p×m的两个小矩阵，计算的时候也变成了两阶段乘法。类似地，也可以参数矩阵转移到频域：使用离散余弦变换（discrete cosine transform，dct）进行压缩，使用反离散余弦变换（idct）进行解压缩。类似的，对卷积层也可以进行分解。

## teacher student

经过模型压缩之后的神经网络有时候可以获得和原始网络几乎一样的质量，因此有人尝试直接以低精度进行训练，或者直接使用更少的参数来训练模型，但是这样的做法通常无法获得原来那样高的质量。而稍微变换一下的方法，使用一个简单的神经网络（student）来拟合（mimic）一个复杂的神经网络（teacher）的输出，而不是直接拟合原始数据，却能够获得很好的效果。高质量的神经网络除了学习到原始数据的各种特征之外，还可以认为它对原始数据进行了“筛选”和“变换”。当数据中含有噪音、错误信息的时候，teacher已经在一定程度上进行了校正。同时，teacher对原始数据进行了一些转换，使得他们的标签不再是那么“硬”。因此使用student去拟合teacher的输出，比让student去拟合原始数据要更加简单。同时，如果student能够很好地“学习”teacher的输出，那么它在原始数据上的表现也不会差。

## 二值/三值网络

将“压缩”做到极限，有人提出了二值（binary）网络和三值（ternary）网络。从字面上理解，将网络的权重和梯度限制在1bit，或者在{-1, 0, 1}之中。因此从设计上占用的空间极低，并且对“计算”本身的需求很低，不再需要复杂并且耗时的乘法，替代成了加法，甚至是位运算（bit operation）。

## Winograd

这是一种用来对卷积层的加速方法。卷积运算在经过快速傅里叶变换（FFT）之后可以变成矩阵相乘，Winograd方法可以进一步减少乘法的数量。FFT方法更加适用于较大的卷积核，而在更小的（例如3*3）情况下，Winograd更加有效。

