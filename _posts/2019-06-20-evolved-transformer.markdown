---
layout: post
title: "The Evolved Transformer"
categories: mypost
---

# The Evolved Transformer

## 论文介绍

* 文章标题：The Evolved Transformer
* 作者：David R. So, Chen Liang, Quoc V. Le
* 发表于：ICML2019
* 参考：https://arxiv.org/abs/1901.11117

## 摘要

近期的一些工作展示了Transformer架构在序列（sequence）任务上的强大能力。同时，神经网络结构搜索（neural architecture search，NAS）也开始超越人工设计的模型。本文的目的是将NAS进行应用，搜索到一个更好的可以替代Transformer的结构。作者们首先构造一个大的搜索空间，利用近期的前向序列模型（feed-forward sequence model）的一些改进，然后运行进化（evolutionary）的结构搜索方法，用最开始的Transformer结构进行热启动（warm start）。为了能在非常消耗算力（computationally expensive）的WMT2014英语德语的翻译任务上，作者们提出了一种Progressive Dynamic Hurdles方法，该方法可以动态分配更多的资源到更有希望的候选模型上。在作者们的实验中找到的结构，称为Evolved Transformer，在以下4个语言任务上稳定超越了Transformer：WMT2014英语德语，WMT2014英语法语，WMT2014英语捷克语，以及LM1B。在大模型上，Evolved Transformer在WMT2014英语德语任务上，创造了新的BLEU得分29.8的纪录。在较小的模型上，获得了和原始Transformer的大模型相同的质量，但是参数数量减少了37.6%，并且在适合移动设备（mobile-friendly）的大约有7M个参数的模型上，比Transformer提升了大约0.7的BLEU分值。

## 1 介绍

在过去的几年中，神经网络结构搜索领域下已经获得了很多令人印象深刻的进步。强化学习（reinforcement learning）和进化算法都证明了有能力产出超越人类设计的模型。这些进步多数都是专注于提升视觉（vision）模型的，尽管也有一些工作在序列模型上进行了研究。在这些场景下，通常都是找到改进的循环神经网络（recurrent neural network，RNN），而RNN在很长一段时间以来都是作为序列问题的事实网络模型。

尽管如此，近期的一些工作显示了在处理序列问题上，有一些比RNN更好的选择。由于基于卷积（convolution）的网络的成功，例如带卷积的序列到序列模型（convolution seq2seq），以及全注意力（full attention）模型，例如Transformer，前向网络称为了一个解决序列到序列（seq2seq）任务的一个可行的选择。前向网络的主要力量来源于它们的计算速度很快，并且相对于RNN来说更容易训练。

本文的主要工作在于，在seq2seq任务中，使用神经网络结构搜索的方法来设计更好的前向网络结构。特别的，作者们使用了锦标赛选择（tournament selection）的结构搜索方法，以及用Transformer进行热启动（因为它是目前领先的并且广泛应用的），来进化得到一个更好的并且更高效率的结构。为了做到这一点，作者们构建了一个搜索空间，反映出了近期的在前向seq2seq模型上的一些改进工作，并且发明了一个叫做Progressive Dunamic Hurdles（PDH）的方法，可以让搜索过程直接在消耗算力很大的WMT2014英语德语翻译任务上进行。作者们的搜索产出了一种新的结构，称为Evolved Transformer（ET），显示了在4个语言任务上的相对于原始的Transformer的稳定提升。这4个任务是：WMT2014英语德语，WMT2014英语法语，WMT2014英语捷克语，以及10亿词汇语言模型（1 billion word language model benchmark，LM1B）。在大模型上，新提出的Evolved Transformer在WMT2014英语德语的任务上获得了BLEU分值29.8的新纪录。同时，在更小的模型上也更高效，和原始的更大的Transformer模型质量相同的情况下减少了37.6%的模型参数，并且在一个适用于移动设备的大约7M个参数的模型上超过Transformer约0.7的BLEU得分。

## 2 相关工作

在很长一段时间以来，在需要用神经网络来进行序列建模（sequence modeling）的时候，RNN都是默认的选择，而LSTM和GRU则是最受欢迎的结构。但是近期的一些工作显示，RNN并不是构造领先的序列模型的必要条件。例如，设计出来的一些高性能的卷积网络，例如WaveNet，Gated Convolution Networks，Conv Seq2Seq，以及Dynamic Lightweight Convolution。也许在这个方向上最有希望的是Transformer结构，它只依赖一个多头（multi-head）注意力（attention）来传递（convey）空间（spatial）信息。在本文的工作中，作者们在搜索空间中同时使用了卷积和注意力机制，来利用这些类型的层（layer）的优势力量。

近期的在序列前向网络上的提升，并不仅仅局限于结构设计。有很多方法，例如BERT和预训练技术，都展示了如何能让类似Transformer的模型超越了RNN预训练。在特定的翻译任务上，提升批处理（batch size）的大小，使用relative position representation，使用weighting multi-head attention，都将WMT2014英语德语，以及英语法语的指标向前进行了推进。但是，这些方法和本文的工作是正交的（orthogon），因为本文只关注于网络结构本身，而不是那些提升整体模型性能的技术。

神经网络结构搜索领域也是近期不断获得成功。效果最好的结构搜索方法都是非常消耗计算资源的。其它也有一些专注于加速的方法，例如DARTS，ENAS，SMASH，SNAS。这些方法将运行每一次搜索的时间大大降低，方法是对每一个候选模型进行了性能的估计，而不是单独对每一个候选模型都投入资源进行完整的训练和评估。但是这些方法在本文的问题上进行实际应用的时候有一些缺点：（1）很难把Transformer结构在这些方法上进行热启动，而这一点是作者们发现获得强结果的必要条件。（2）ENAS和DARTS在作者们需要搜索的结构上，需要消耗太多的内存。（3）在视觉领域上效果最好的模型，例如AmoebaNet，使用进化的NAS搜索得到的，而不是这些高效率的方法。作者们优化的是最好的结构而不是最好的搜索效率。

有一些方法尝试同时提升效率（对候选模型的效果进行估计）和最大化搜索质量（在必要的时候讲模型训练到底），一些例子是：有一些工作使用了Hyperband，以及PNAS使用了代理（surrogate）模型。作者们在本文中提出的Progressive Dynamic Hurdles方法和上述方法有一些相似之处，作者们将最好的模型训练到底，但是提升效率的方法是将希望不大的模型尽早扔掉。但是，和其他一些有一拼（comparable）的算法（比如Hyperband和Successive Halving）非常不同的是，新的方法可以让进化算法动态选择新的有希望的候选，而Hyperband和Successive Halving则是先建立了候选模型的池子。在本文的节5中，作者们会说明在他们的大的搜索空间中，这些方法是无效果/低效率（ineffective）的。

## 3 方法

作者们引入基于进化方法的结构搜索，因为它简单，并且已经有工作证实，在资源受限的情况下，该方法比强化学习的方法要更加高效。作者们用和前人类似的锦标赛选择算法，但是将年龄的正则化（aging regularization）忽略掉了，鼓励读者们去仔细研究该方法的细节。考虑到节约空间，作者们在这里只是给出一个算法的简要的回顾。

锦标赛选择进化结构搜索（tournament selection evolutionary architecture search），首先要定义基因编码（gene encoding），用来对一个神经网络结构进行描述。作者们在下面的搜索空间一节描述他们的编码方法。构造一个初始的人群（population），通过在基因编码的空间中进行随机采样（random sample）来构造个体（individual）。这些个体，每一个都对应于一个网络结构，会用来训练，并且获得适合度（fitness），在目前的情况下就是该模型在WMT2014英语德语的验证集上的负对数复杂度（negative log perplexity）。整个种群然后将不断地进行采样（sample），产生了一些子群（subpopulation），适合度最高的那些个体们将被选择成为父母（parent）。选出来的父母的基因编码发生变异（mutate），即，某些编码的域将随机发生变化成不同的值，然后生成了孩子模型（child model）。这些孩子模型将继续用在目标任务上进行训练和评估的方式，获得适合度，正如同最开始的种群一样。当所有的适合度评估结束了之后，整个种群从头再来一遍进行采样，在子群里面适合度最低的个体将被杀死（kill），也就是从种群中移除（remove）。新的被评估过的孩子模型将加入到种群中，占据之前被杀死的个体的位置。整个过程将反复进行，最后会得到一个全部由高适合度的个体组成的种群，在本文中则是表现好的网络结构。

### 3.1 搜索空间

作者们的编码搜索空间，收到NASNet搜索空间的启发。但是进行了修改（alter）， 使得它能够表现在近期领先的前向seq2seq的网络的结构特性。至关重要的是，作者们保证了搜索空间可以表示Transformer结构，这样才能用它来进行种群的初始化。

搜索空间包含两个可以堆叠（stackable）的单元（cell），一个是编码器（encoder），一个是解码器（decoder）。每个单元包含类似于NASNet的块（block）：接收两个隐藏状态的输入，生成一个新的隐藏状态作为输出。编码器包含6个块，解码器包含8个块。这样的话Transformer结构可以被准确表示出来。对于每一个输入，这些执行不同的变换的块，会将所有的变换的输出都连接在一起，产生一个单独的块的输出。作者们将这些对每一个输入进行的一系列变换，称之为一个分支（branch）。作者们的搜索空间包含了5个分支级别的搜索域（输入，归一化，层，输出维度，激活），一个块级别的搜索域（合并的函数），一个单元级别的域（单元的个数）。

在搜索空间中，一个孩子模型的基因编码可以表示为：（左输入，左归一化，左层，左相对输出维度，左激活，右输入，右归一化，右层，右相对输出维度，右激活，合并函数）* 14+（单元个数）* 2。前6个块是分配给编码器，后8个块分配给解码器。在本文的补充材料中提供了字典，可以看到搜索空间是7.30*10^115个模型，尽管作者们应用了一些限制（更多的细节也在补充材料中）。

### 3.2 用Transformer给搜索空间作为种子

在前人的一些神经网络结构搜索的工作中，依赖于手工设计的搜索空间。本文作者们特意让搜索空间尽量减少手工的负担，强调自动化的搜索方法。为了帮助在大的搜索空间中游走（navigate），作者们发现用一个强的模型对初始化的搜索空间作为种子，也就是热启动，会比较简单。在目前的问题中就是使用Transformer。这个使得整个搜索从一个好的出发点开始，并且保证了在一代一代的过程中，种群中至少有一个单独的强的父母。在文章的实验结论部分将会给出对以上说法的对应的实验结论。

### 3.3 使用Progressive Dynamic Hurdles的进化

作者们使用的进化算法，是从锦标赛选择进化结构搜索中拿来的，如之前的介绍。和前人的工作不同的是，他们的搜索是在CIFAR10数据集上，而本文作者们的搜索则是在一个明显更大的训练和评估集上的。具体来说，要训练一个Transformer到顶峰的效果，在WMT2014的英语法语数据集上，大约需要300K个训练步数，或者10小时，使用一个单独的谷歌TPUv2芯片。相反的是，前人使用的是资源较少的CIFAR10数据集，大约需要2个小时的训练时间，并且是一个相对于ImageNet数据集的一个好的代理（proxy）。但是，在作者们的初步试验中，发现并不能找到一个代理任务，可以给出一个足够强的信号来说明每个孩子模型可以在WMT2014英语法语任务上能做到多好。作者们用的是数据集中的一部分然后多种形式的比较激进的提前终止（early stopping）。

为了解决这个问题，作者们形式化了一个方法，来动态分配资源，根据适合度来对把资源分配给最有希望的网络结构。这个方法作者们称之为Progressive Dynamic Hurdles，可以让那些一直表现很好的模型多训练几轮。最开始的时候，和普通的带提前终止的锦标赛选择进化搜索方法一样，每个孩子模型先训练一个相对较少的s0步骤，在用来评价适合度之前。但是，当一个事先确定的孩子模型的个数，m，已经评价了以后，一个栏板（hurdle），h0，根据当前的种群的平均适合度给创建出来。对于从此以后再生成的m个孩子模型来说，所有那些已经训练了s0并且获得了一个比h0的适合度还要高的模型，他们可以再训练s1轮并且再次进行评估以获得最终的适合度。当另外的m个模型都用这种方法进行处理之后，另一个栏板h1创建出来，用当前的种群（已经训练了最大轮数）中的所有成员的平均适合度来进行计算。再对于其后的m个孩子模型，训练和评估过程照此办理，只不过在训练了s0+s1步数之后，适合度超过h1的模型，会被继续给于s2轮的训练，然后才会去进行评估以获得最终的适合度。这个过程将一直反复进行，直到达到了满意的最大训练轮数。算法1（见原文的补充材料）描述了如何根据栏板计算每个个体模型的适合度。算法2（见原文的补充材料）描述了修改之后的利用Progressive Dynamic Hurdles的锦标赛选择方法。

尽管不同的孩子模型在获得它们自己的最终的适合度之前，会经过不同数量的迭代轮数，但是这并不影响适合度这个指标的可比性。锦标赛选择进化算法只关心相对的适合度排名，然后用来选择哪些子群中的成员将被杀掉或者是成为下一代的父母。一个候选人比其他成员好或者坏的间距并不起作用。假设所有模型在训练中都没有出现过拟合（这也是作者们在实验中发现的实际情况），然后他们的适合度随着分配给他们的训练轮数单调递增，两个孩子模型之间的比较，可以看成是，在这两个孩子模型的累加的训练轮数的较低的那一个的适合度的比较。根据对应栏板的定义，那个训练了更多轮数的模型，在训练轮数较少的时候一定是超过了栏板的线。而训练了较少轮数的模型，在轮数较少的时候一定是没有超过栏板的线。所以同样是在较少的训练轮数的地方卡的话，那个训练总数较多的模型一定是更好的。

修改了之后的适合度算法的优点在于，表现不好的孩子模型在评估了适合度之后，就不会再消耗那么多的计算资源了。当一个候选模型的适合度低于一个可忍受的值的时候，它的评估过程会马上终止。这种方式有可能导致一个本来好的模型（仅仅在训练的最后阶段才表现出来）会标记成不好的模型。但是由于会将很多的不好的模型扔掉，节约下来了很多资源，使得整体的质量上升，也就足够纠正可能的对好模型的损失。作者们的实验支持了这样的观点。


