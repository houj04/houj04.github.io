---
layout: post
title: "MnasNet: Platform-Aware Neural Architecture Search for Mobile"
categories: mypost
---

# MnasNet: Platform-Aware Neural Architecture Search for Mobile

## 论文介绍

* 文章标题：MnasNet: Platform-Aware Neural Architecture Search for Mobile
* 作者：Mingxing Tan，Bo Chen，Ruoming Pang，Vijay Vasudevan，Mark Sandler，Andrew Howard，Quoc V. Le
* 发表于：https://arxiv.org/abs/1807.11626

## 摘要

设计在移动设备（mobile device）上专用的卷积神经网络（convolutional neural network，CNN）是很有挑战性的， 因为移动设备上的模型需要小和快，并且仍然准确。尽管已经有很多从各个维度来设计和改进移动的卷积神经网络的方法，当需要手工进行权衡的时候仍然很难，因为有太多的网络结构的可能需要考虑。在本文中，作者们提出一种新的自动化移动神经网络结构搜索的方法（MNAS），可以显式地将模型的延迟（latency）考虑到优化目标（objective）中，这样搜索过程中就可以找到这样的模型，使得它具有准确率和延迟的良好的权衡。和前人的工作不同的是，这里的准确率，之前的做法通常是一个不够准确的方法（如，FLOPS），而作者们提出的方法是直接衡量现实世界中进行推理（inference）的延迟，做法是在手机上运行模型。为了更进一步针对灵活性和搜索空间的平衡，作者们提出了一种新的分解的分层的搜索空间，可以在整个网络中鼓励（encourage）层（layer）的多样性（diversity）。实验显示作者们提出的新方法在多个视觉（vision）任务中稳定超过现有的移动卷积神经网络最好水平。在ImageNet分类任务中，作者们的MnasNet获得了75.2%的top-1准确率，在一台Pixel手机上的延迟是78毫秒，比MobileNetV2要快1.8倍并且准确率高0.5%；比NASNet速度快2.3倍并且准确率高1.2%。在COCO目标检测（object detection）中，MnasNet在mAP指标上也比MobileNets要好。


## 1 介绍

卷积神经网络在图像分类（image classification）和目标检测和其它很多应用中都起到了重要的作用。流行的卷积神经网络模型在不断变得更深和更大，进而也变得更慢，需要更多的计算。这种在计算量上的需求的增加，导致了很难将领先的卷积神经网络模型部署在资源受限的平台上，例如移动设备或者嵌入式（embedded）设备。


在给定移动设备上受限的计算资源的情况下，近期有一些研究工作关注于对移动的卷积神经网络进行设计和改进，主要方法是减少网络结构的层数以及使用更轻量的计算，例如depthwise convolution和group convolution。尽管如此，设计一个资源受限的移动模型还是很有挑战的：设计者需要仔细平衡准确率和资源利用率，这会导致一个非常大的设计空间。

在本文中，作者们提出了一种自动化的网络结构搜索方法，可以用来设计移动的卷积神经网络模型。原文的图1展示了作者们的方法的整体视图，和前人的方法的主要区别是考虑了延迟的多目标奖励（multi-objective reward），以及新的搜索空间。该方法包含两个主要的思想。第一个，将设计问题形式化为一个多目标的优化问题，同时考虑了准确率和推理延迟。和前人的工作不同的是，他们使用FLOPS来对推理延迟进行近似，而作者们直接将模型在真实的移动设备上进行执行，获得真实世界上的延迟。这个思路来源于这样的发现：FLOPS经常是一个不准确的指标：举例来说，MobileNet和NASNet有相似的FLOPS（575M vs 564M），但是它们的延迟差距非常大（113毫秒 vs 183毫秒）。第二个，作者们发现前人的自动化方法，主要是搜索少数几类计算单元（cell），然后将相同的计算单元重复叠加（stack）以构成网络。这种做法将搜索过程变得简单，但是也阻止了层的多样性，而这种多样性对计算效率是很重要的。为了解决这个问题，作者们提出了一种新的分解的层次搜索空间，允许各个层可以在架构上不同，然后还能获得灵活性以及搜索空间大小的平衡。

作者们将提出的方法应用于ImageNet分类和COCO目标检测。原文中的图2总结了作者们提出的MnasNet和其它优秀移动模型的各种比较。和MobileNetV2相比，作者们的模型将ImageNet的准确率提升了3.0%，在Google的Pixel手机上的延迟接近。另一方面，如果限制目标准确率，那么MnasNet可以比MobileNetV2快1.8倍，比NASNet快2.3倍并且有更好的准确率。和广泛使用的ResNet-50相比，MnasNet模型获得了稍微高一点（76.7%）准确率，但是模型参数少4.8x，并且乘加（multiply-add）操作少了10x。将作者们的模型作为一个特征提取器（feature extractor）加到SSD目标检测框架中，模型同时提升了推理延迟以及mAP指标在COCO数据集上，对比的是MobileNetsV1和MobileNetsV2，并且获得了和SSD300相比接近的mAP指标（23.0 vs 23.2），但是乘加操作减少了42x。

整体来说，作者们的主要贡献是：

1、作者们提出了一个多目标的神经网络结构搜索方法，可以同时优化准确率以及移动设备上的真实世界的延迟。

2、提出了一个新的分解的分层的搜索空间，可以让层有多样性，同时可以保留灵活性和搜索空间大小的平衡。

3、在典型的移动延迟限制下，展示了新的最优的模型准确率，同时在ImageNet分类和COCO目标检测任务中。

## 2 相关工作

提升卷积神经网络模型的资源利用率，是近几年的一个活跃的研究课题。部分通常使用的方法包括：1）将一个基线模型的权重（weight）和/或激活（activation）进行量化（quantize），获得低比特的表示。2）根据FLOPS将不重要的滤波器（filter）剪枝掉（prune），或者根据具体平台来根据延迟进行剪枝。尽管如此，这些方法都是绑定在一个基线模型上，并且并没有关注于学习新的卷积操作的组合。

另一类常见的做法是直接手工生成更加高效的移动的模型结构：SqueezeNet使用低损耗的1x1卷积以及更小的滤波器，来降低总的参数个数以及计算量。MobileNet大量使用depthwise separable convolution来最小化计算密度。ShuffleNets利用了低损耗的group convolution以及channel shuffle。Condensenet学习跨层的group convolution的连接。最近，MobileNetV2获得了领先的结果，利用了资源利用率高的inverted residuals以及linear bottlenecks。不幸的是，由于有潜在的非常大的设计空间，上面这些手工设计的模型通常需要可观的人力工作。

近期以来，有很多研究的关注点到了用神经网络结构搜索来进行自动化模型设计上。这些方法主要是基于强化学习（reinforcement learning），进化（evolution）搜索，可微（differentiable）搜索，以及其它的一些学习算法。尽管用“将搜索出来的少数几个计算单元重复叠加”的方法可以生成移动端大小的模型，这些方法都没有将移动平台的限制纳入搜索的过程或者搜索空间之中。和作者们的工作比较接近的有MONAS，DPP-Net，RNAS以及Pareto-NASH。这些尝试在搜索卷积神经网络的同时，去优化多个目标，例如模型大小和准确率。但是这些方法的搜索过程是在小数据集（CIFAR）上面的。作为对照，本文定位于真实世界的移动延迟限制，并且着眼于更大的任务例如ImageNet分类以及COCO目标检测。

## 3 问题的公式化

作者们将设计的问题转换成一个多目标的搜索，目的在于寻找卷积神经网络模型，同时具有高准确率和低的推理延迟。和之前的结构搜索不同，他们优化的是间接的度量，例如FLOPS，而作者们考虑直接的真实世界的推理延迟，方法是在真的移动设备上运行卷积模型，然后把这些真实世界的推理延迟加入到目标之中。这样做就直接度量了在实际中能获得什么：作者们早期的实验显示，对真实世界的延迟进行近似是困难的，因为移动硬件和软件各个有不同的特点。

给定一个模型$m$，用$ACC(m)$来表示在目标任务上的准确率，用$LAT(m)$表示在目标移动平台上的推理延迟，用$T$来表示目标延迟。一个常见的方法是把$T$当成是一个硬限制（hard constraint），并且在这个限制之下最大化准确率：

$$\text{maximize}_m ACC(m), \text{ subject to }LAT(m) \le T$$

但是，这种方法只能最大化一种度量指标，不能提供多个帕累托最优解（Pareto optimal）。非正式地说，一个模型被称之为帕累托最优，要么它是不增加延迟的情况下具有最高的准确率，要么它是不降低准确率的情况下具有最低的延迟。给了结构搜索的计算消耗，更关心的是在单次结构搜索中找到多个帕累托最优解。

历史上有多种方法，作者们用了一种自定义的带权重的乘积的方法，来近似帕累托最优解，要优化的目标是：

$$ \text{maximize}_m ACC(m) \times \left[ \dfrac{LAT(m)}{T} \right]^w$$

这里的$w$是权重因子，定义是：

$$ w=\begin{cases}
\alpha, & \text{if  } LAT(m) \le T \\
\beta, & \text{otherwise}
\end{cases}$$

上式中的$\alpha$和$\beta$是针对具体应用设置的不同常数。选择$\alpha$和$\beta$的一个经验的规则是，保证帕累托最优解在不同的准确度-延迟的平衡上具有相似的奖励。

举例来说，经验上发现：将延迟加大一倍，通常会获得准确率的相对上升5%。现在有两个模型：（1）M1模型，延迟是$l$，准确率是$a$；（2）M2模型，延迟是$2l$，准确率高5%，也就是$a\cdot(1+5\%)$。这两者应该具有相似的奖励，也就是$Reward(M2) = a \cdot(1+5\%)\cdot(2l/T)^\beta \approx Reward(M1) = a \cdot (l/T)^\beta$。把这个式子求解得到$\beta\approx -0.07$。因此，作者在后续的实验中使用$\alpha=\beta=-0.07$，除非另有说明。

原文中的图3展示了两组典型的$(\alpha,\beta)$相关的目标函数。靠上的一张图是$(\alpha=0, \beta=-1)$，为了简便，认为当测量出来的延迟小于目标延迟$T$的时候，目标值就是准确率，否则的话，对目标值进行尖锐的惩罚，用来不鼓励模型超出延迟的限制。靠下的一张图是$(\alpha=\beta=-0.07)$，把目标延迟$T$看成软限制，然后根据测量得到的延迟值缓慢平滑调整目标值。

## 4 移动网络结构搜索

在这一节，首先讨论作者们提出的新的分解的分层搜索空间，然后总结基于强化学习的搜索算法。

### 4.1 分解的分层搜索空间

在近期的某些研究中所展示的那样，一个well-defined搜索空间，对于神经网络结构搜索来说是极端重要的。但是，多数的之前的研究，只是搜索一些复杂的计算单元，然后重复将同样的单元堆叠起来。这类方法不能做出层的多样性，而这一点恰是作者们需要展示的，对于提升准确率和低延迟很重要。

和前人的方法不同的是，作者们提出了一种新的分解的分层的搜索空间，把一个卷积神经网络模型拆分成不同的块（block），然后每个块分别搜索操作（operation）和连接（connection），进而在不同的块中可以含有不同的层的结构。直觉上，需要根据输入和输出的形状来搜索最好的操作，获得最好的准确率-延迟的权衡。举例来说，一个卷积神经网络的前面几个阶段，通常处理更大量的数据，因此对整体延迟的影响更大（相对于后面的阶段来说）。正式一点，把广泛使用的depth wise separable convolution核的四元组，记作$(K,K,M,N)$，它把一个形状是$(H,W,M)$的输入，转换成形状是$(H,W,N)$。其中$(H,W)$是输入的分辨率，而$M$和$N$分别是输入和输出的滤波器的形状。那么所有的乘加操作的总数可以写成：

$$H*W*M*(K*K+N)$$

这里，如果总的计算量是受限的，那么需要仔细平衡核的大小$K$和滤波器大小$N$。举例来说，用更大的核大小$K$以增强receptive field，必须要用减小同一层滤波器的大小$N$或者从别的层计算，以进行平衡。

原文中的图4展示了搜索空间的基础结构。将一个卷积神经网络切分成一个预先定义好的块的序列，最终减少了输入的分辨率然后增强了滤波器的大小，正如很多卷积神经网络中常见的那样。每一个块里面有一系列相同的层，它们的操作和连接是用“每个块都不同”的“子搜索空间”决定的。特别的，对第$i$个块的子搜索空间包括以下的选择：

卷积操作ConvOp：正常的卷积（conv），depthwise的卷积（dconv），以及mobile inverted bottleneck conv。

卷积核的大小KernelSize：3x3，5x5。

Squeeze-and-excitation ratio（SERatio）：0，0.25。

跳操作SkipOp：池化（pooling），identity residual，或者没有跳。

输出过滤器大小$F_i$。

块内部的层数$N_i$。

ConvOp、KernelSize、SERatio、SkipOp、$F_i$这几个决定了层内部的结构，而$N_i$决定了一个块里面这个层要重复多少次。举例来说，原文中的图4里面的块4，里面的每一个层，都含有一个inverted bottleneck 5x5卷积，以及一个identity residual skip path，这样的层一共重复$N_4$次。作者们参考了MobileNetV2，将所有的搜索项目都进行了离散化：对每个块中的层的数量，基于MobileNetV2，设置为{0,+1,-1}；对每个层的滤波器的大小，作者们搜索的是“相对于MobileNetV2的比例”，可以是{0.75, 1.0, 1.25}。

作者们提出的分解的分层搜索空间，有一个独有的优势，可以平衡各个层的多样性以及完整的搜索空间的大小。假设将网络切分成$B$个块，每个块都有一个大小是$S$的子搜索空间，每个块里面平均有$N$个层，那么总的搜索空间的大小则是$S^B$，而普通的每层都搜索的空间大小则是$S^{B*N}$。一个典型的例子是$S=432,B=5,N=3$，用作者的方法，搜索空间的大小大约是$10^{13}$，而普通的每个层搜过去的空间大小则是$10^{39}$。

### 4.2 搜索算法

受到某些近期工作的启发，作者们使用强化学习（reinforcement learning）的方法来找到多目标搜索问题的帕累托最优解。选用强化学习的原因是它方便并且奖励可以很容易进行自定义，但是其它方法（例如进化）应该也可以使用。

具体来说，作者们参考了文献36的做法，将搜索空间中的每一个卷积神经网络的模型结构映射成了符号的列表（list of tokens）。这些符号是通过强化学习的智能体（agent）基于它自身的参数$\theta$生成的一系列动作（action）$a_{1:T}$而决定的。目标是最大化奖励的期望（maximize the expected reward）：

$$J=E_{P(a_{1:T};\theta)}[R(m)]$$

上式中，$m$是用动作$a_{1:T}$采样（sample）出来的模型，$R(m)$是用式2定义的目标值。

如原文中的图1所示，搜索的框架包括三个组件：一个基于循环神经网络（recurrent neural network，RNN）的控制器（controller），一个用于获得模型准确率的训练器（trainer），以及一个基于手机的推理引擎，用来衡量延迟。作者们用众所周知的“采样-评价-更新（sample-eval-update）”的循环来训练控制器。在每一步中，控制器首先使用它当前的参数$\theta$来采样出一个小批量（batch）的模型：根据其内部的RNN的softmax logits来预测出一系列的符号。对于每一个采样出的模型$m$，在目标任务上进行训练，以获得准确率$ACC(m)$，并且在真实的手机上运行以获得推理延迟$LAT(m)$。然后用式2计算奖励值$R(m)$。在每一步的末尾，使用式5来计算期望的奖励，使用Poximal Policy Optimization来让它最大化，并且更新控制器的参数$\theta$。这种“采样-评价-更新”的循环一直进行，直到到了最大的步数，或者参数$\theta$收敛为止。












