---
layout: post
title: "DSD training"
categories: mypost
---

## 论文介绍

* 文章标题：DSD: Dense-Sparse-Dense training for deep neural networks
* 作者：Song Han, Huizi Mao, Enhao Gong, Shijian Tang, William J. Dally
* 发表于：ICLR 2017

## 摘要

现在流行的深度神经网络（deep neural network）通常都有非常多的参数（parameter），导致很难训练。作者们提出了一个叫做DSD的方法，分三个阶段，对深度神经网络进行正则化（regularize），获得了更好的效果。首先是一个D（dense）阶段，训练一个稠密的网络，学习每个连接（connection）的权重（weight）和重要性（importance）。在后续的S（sparse）阶段，对权重较小的（也就是不重要的）网络连接进行剪枝（prune），并且保持这种稀疏的限制的同时对网络进行重训练（retrain）。在最后的D（dense）阶段，作者们去掉了稀疏性的限制，恢复了模型的容量，把之前剪枝掉的权重重新初始化为0，然后重新调整整个的稠密网络。从实验结果上看，DSD三阶段训练过程可以把多种CNN、RNN、LSTM不同类型的网络，在图像识别、自动生成标题、语音识别等等多种不同的任务上，都提升效果。

在实践中DSD很容易使用：训练的时候，DSD只增加了一个超参数（hyper parameter），即S阶段的稀疏比例。在预测的时候，DSD不改变网络结构，也不会引入任何其他多余的操作。这种一致性，以及明显的效果提升，说明了目前训练方法对于找到局部最优点的不足，而DSD可以很有效地找到更好的解。

## 1. 介绍

深度神经网络在很多应用领域都体现出了很强的效果提升，从计算机视觉，到自然语言处理，和语音识别。各种强大的硬件设备使得训练复杂的神经网络成为可能。复杂网络的优点在于，它们有很强的表示能力，能捕捉到输入特征（feature）和输出之间的非线性关系。缺点在于，这么大的模型，网络也能同时把输入数据中的噪音给学进来。这部分训练集中的噪音是无法在新的数据集上进行泛化的，会导致过拟合（over-fitting）和高方差（variance）。

相反地，如果只是简单降低模型容量，就会走向另外一个极端，结果会让一个机器学习系统，丢失从输入特征到输出目标的重要关系，导致欠拟合（under-fitting）和高偏差（bias）。偏差和方差很难同时优化。

为了解决上面提出的问题，作者们提出了一个三阶段训练流程，称为DSD。先用传统的训练方法训练一个稠密模型，然后用限制稀疏性的方法对模型进行正则化，最后将被剪枝的权重恢复以提升模型容量。在测试时，DSD训练最终得到的模型从结构上和维度上都和原始的稠密模型完全一致，没有引入任何的多余运算。

## 2. DSD训练流程

作者们提出的DSD训练包含三个流程：稠密、稀疏、重新稠密。具体的流程可以参考原文中的图1以及算法1。

### 2.1 最初的稠密训练

第一个D阶段和正常的模型训练一样，学习每个连接的权重和重要性。和传统训练稍有不同的是，D阶段训练的目标，不仅仅是学习权重的值，同时还要学习到哪些连接是重要的。作者们用了简单的启发式（heuristic）策略：根据权重的绝对值，来量化它们的重要性。

### 2.2 稀疏训练

S阶段把权重较低的连接给剪掉，训练一个稀疏的网络。所有的层都要添加一个相同的“稀疏性”，所以需要增加一个超参数，稀疏度（sparsity），即，有多少比例的权重被剪枝到0。

对每个层（layer）$W$，它含有$N$个参数，按照参数进行排序，选择第k大的$\lambda=S_k$作为阈值，这里$k = N * (1-sparsity)$，进而可以生成一个0到1的掩码（mask），把所有权重比$\lambda$小的都清理掉。

作者们移除较小权重的理由是基于泰勒展开式（Taylor expansion）。损失函数（loss function）和它的泰勒展开式由公式（1）和（2）给出。

$$ Loss = f(x, W_1, W_2, W_3, \dots)$$

$$ \Delta Loss = \dfrac{\partial Loss}{\partial W_i} \Delta W_i+ \dfrac{1}{2} \dfrac{\partial^2 Loss}{\partial W_i^2} \Delta W_i^2 + \dots$$

在对权重进行限制时，希望可以减少损失函数的上升，所以也就需要将公式（2）中的第1项和第2项最小化。

由于剪枝操作的本质是将某些权重置零，所以$\Delta W_i$其实就是$W_i - 0 = W_i$。注意局部最优点附近，有$\partial Loss / \partial W_i \approx 0$，并且$ \partial^2 Loss / \partial W_i^2>0$，所以只有二阶项起作用。计算二阶导数$ \partial^2 Loss / \partial W_i^2$的代价很高，所以直接用$ \|W_i\|$作为剪枝的条件。$ \|W_i\|$的绝对值越小，说明对损失函数的上升的影响也越小。

在重训练过程中，每一轮迭代都使用0-1的mask，就将一个稠密的网络转换成了稀疏的网络。并且，在稀疏的限制条件下，这个稀疏的网络能完全反应原始网络的准确度，甚至某些情况下准确度还略高于原始的网络。新增加的超参数“稀疏率”对所有层都配置成一样的，并且可以通过验证集（validation）来进行调整。在作者们的实验中，他们发现将稀疏度设置为25%到50%之间比较合适。

### 2.3 最后的稠密训练

最后的D阶段训练，将之前剪枝掉的连接进行了恢复，将网络重新还原成稠密的。之前被剪枝掉的连接，被初始化为权重0，然后对整个网络进行重训练，学习率设置为初始学习率的$1/10$（因为稀疏网络应该已经停在一个好的局部最优点了）。其他的超参数，例如dropout和weight decay等等都不变。由于恢复了被剪枝的连接，最后这个D阶段让模型的容量上升，进而可以收敛到比上一个S阶段更好的局部最优点。

作者们还绘制了各个阶段的权重分布图，可参考原文。作者们用的是GoogLeNet inception_5b3x3，但是这种分布十分具有代表性，和VGGNet以及ResNet都差不多。

最开始的权重分布很明显是中心在0点，两边的尾部都下降得很快。剪枝操作是基于绝对值的，所以剪枝之后，分布最中间的一大块都没有了。在重训练阶段，网络参数自己调整成了“软”边界，变成了一个双峰分布（bimodal distribution）。在最后的D阶段，首先将剪枝掉的权重初始化成0。最后，所有的权重，不管是否被剪枝过，一起进行重训练，保留超参数不变。

把图d和图e进行对比，发现从没有被剪枝过的权重的分布几乎保持不变，而剪枝过的权重也还大量处在0附近。整个网络的权重分布的绝对值的均值整体变小了，这是一个好现象。

## 3. 相关工作

Dropout（将神经元的输出随机置为0）和DropConnect（将神经元之间连接的权重随机置为0)：DSD、Dropout和DropConnect，这三种方法都可以对神经网络进行正则化和避免过拟合。不同之处在于，Dropout和DropConnect在每次随机梯度下降（SGD）的迭代时，是用的随机稀疏化的模式，而DSD 则是在用数据驱动的方法学到一种确定性的稀疏化方法。作者们在VGG16、GoogLeNet和NeuralTalk上的实验显示，DSD可以和Dropout一起使用。

蒸馏（distillation）：模型蒸馏的方法可以把一个大模型学到的知识转移给小模型，在模型部署上会更高效。这是另外一种在神经网络中，不用修改结构，就能提升效果的方法。

模型压缩：模型压缩（作者们之前提出的方法）和DSD都使用了剪枝。不同之处是，DSD除了在保持模型准确率之外，还能在相当显著的程度上提升准确度。另一个区别是，DSD不需要太过于激进的剪枝，中等程度的剪枝（大约50%到60%稀疏）就已经效果很好了。而之前的模型压缩需要更激进的剪枝以获得高压缩率。

稀疏正则化（sparsity regularization）和硬限（hard thresholding）：关于基于截断的稀疏网络，前人一些工作从理论上分析了高维统计模型的应用。此外还有一些类似的用迭代的硬限以及连接恢复的训练策略。稀疏正则优化在Compressed Sensing中用来寻找反问题的最优解。

