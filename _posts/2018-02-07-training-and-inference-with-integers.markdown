---
layout: post
title: "Training and Inference with Integers"
categories: mypost
---

## 论文介绍

* 文章标题：Training and inference with integers in deep neural networks
* 作者：Shuang Wu, Guoqi Li, Feng Chen, Luping Shi
* 发表于：ICLR 2018

## 摘要

在嵌入式系统（embedded system）中，使用和部署离散参数的深度神经网络（deep neural network，DNN）是个活跃并且有前途的话题。尽管以前已经有了很多的成功经验，将推理（inference）时的精度降低，但是把训练和推理同时转换成低比特（bit）的整数，还没有看到成功经验。在这篇文章中，作者们提出了一种新的方法，称为WAGE，把训练和推理阶段全部离散化。这里的W表示权重（weight），A表示激活（activation），G表示梯度（gradient），E表示误差（error）。为了让定点计算设备能用上纯离散数据流，作者们还把小批量数据归一化（batch normalization）用一个常数缩放层（constant scaling layer）替换掉，并且对其他一些很难用整数实现的组件进行了简化。在一些数据集上可以获得更好的准确率（accuracy），说明WAGE在某种意义上可以看成是一种正则化（regularization）。

从经验上，作者们展示了这样的一种可能性，在某些硬件系统上（例如基于整数的深度学习加速器或者是类神经芯片）部署模型训练，同时可以获得和基线相当的准确率以及更高的能量（energy）效率。而这将是未来人工智能（AI）应用在很多场景下的一个重要因素。

## 1. 介绍

最近一段时间以来，DNN在大量的AI产品上有广泛的应用。由于有海量的可调整的参数，DNN有很强的、多层次的特征提取（feature extraction）和表示（representation）的能力。但是，训练DNN需要消耗很多能源的设备，例如GPU和CPU，它们需要高精度计算（例如32位浮点计算）处理器和很多的内存。这就极大程度上限制了在可移动设备上的应用。更进一步，最新的网络结构通常有更多的参数权重和有效容量，也就更容易导致过拟合（overfitting）。

因此，在推理时如何减少网络的大小，吸引了众多研究者，有些还专注于专用硬件的商业化解决方案。由于在随机梯度下降（staochastic gradient descent，SGD）的优化中，需要求和操作（accumulation），所以训练时候对精度的需求会通常比推理时候的更高。因此，多数的现有技术只关注于，如何将一个已经完全训练好的网络进行压缩和部署。

在本文中，作者们讨论的问题是，如何同时使用低比特数的整数来进行训练和推理，这对于在专有硬件上实现DNN的很重要。作者们考虑以下两个基本问题：（1）如何把所有的操作数（operand）和操作符（operation）进行量化；（2）在SGD计算和求和的时候，需要多少比特和多少状态。

为了解决上面的两个问题，作者们提出了一个框架，叫做WAGE，包括权重（W）、激活（A）、梯度（G）、误差（E），在训练和推理中，所有的层（layer）都用低比特数的整数。

对操作数，用线性映射（linear mapping）以及保留方向不变的变换（orientation-preserved shifting），可以得到三元（ternary）的权重，而激活和梯度累加可以用8或者更少比特的整数。

对操作符，例如batch normalization，替换成一个常量的缩放因子。其他的一些微调（fine-tuning）模型的技术，例如使用动量项（momentum）和L2正则化（regularization）被简化，或者去除，对性能只有很少的影响。考虑到整体两个方向的传播，作者们把推理阶段完全做成了一个数据流，只有求和与比较操作。训练则变成了只有低比特数的带对齐（alignment）的乘法和加法（multiply-accumulate，MAC）。

作者们启发式地探寻了误差计算和梯度求和的整数计算所需要的比特数，这些在前人的工作中是很少讨论的。实验表明，相对值（即，方向，orienation）比绝对值（阶数，order of magnitude）对收敛更加重要。另外，比较小的权重值，在一层一层之间传递的时候就不那么重要，所以在量化的时候可以部分忽略掉。作者们用这些现象，使用了一种方向不变的移位操作来限制误差。对梯度求和的时候，尽管权重在推理时已经变成三元值，在存储和累加梯度更新量的时候仍然要有一个相对高些的比特数。

作者们提出的框架在MNIST、CIFAR10、SVHN、ImageNet这些数据集上进行了实验评估。和只在推理时量化权重和激活相比，提出的新方法具有相似的准确率，并且可以减轻过拟合，显示出了某种类型的正则化。从作者已知的情况来看，WAGE是第一个数据流（前向和后向）完全用低精度整数进行神经网络计算的工作，可以很整洁地应用到专用硬件的训练和预测阶段。对应的代码已经开源到GitHub上面。

## 2. 相关工作

作者们主要研究的是在训练和推理时，降低操作数和操作符的精度。其它的一些减少复杂度方法，例如用模型压缩、剪枝、精简结构等等是很有效的，但是不在这篇文章的讨论范围内。

权重和激活：之前有方法训练二元权重（BC）和激活（BNN）。他们的做法向权重和激活添加了噪音，作为一种正则化。但是实数值的梯度是用实数值变量进行累加的，这说明在SGD优化的时候仍然需要高精度计算。XNOR-Net对每个过滤器（filter）的权重都用了不同的缩放系数来提高效果。在XNOR-Net中的卷积层，可以用XNOR的逻辑单元和bit-count操作来高效实现。但是，这些浮点因子是在训练中同时计算出来的。在TWN和TTQ中，作者们用了两个对称的阈值，来限制权重在三元组$\\{+1, 0, -1\\}$之中，他们指出这是模型复杂度和表示能力的一个折中。

梯度计算和累加：在DoReFa-Net中，他们在反向传播阶段，把梯度量化到了低比特的浮点数和离散的状态。TernGrad把梯度更新量化到了三元组，减少分布式训练中梯度同步（synchronization）带来的额外开销（overhead）。然而，在DoReFa-Net和TernGrad的训练流程中，权重都是用32位浮点存并且用32位浮点计算的。此外，batch normalization以及它的导数都被忽略了。所以整个训练的计算图，仍然是用32位浮点数计算的，并且由于引入了量化，其实操作更复杂了。整体来说，把DoReFa-Net应用于基于整数的硬件上很难，但是它却显示了一种在高维离散空间用离散梯度下降方向的可能性。

## 3. WAGE量化

WAGE量化的主要思想是，把4个操作数限制在低比特数的整型变量：推理时的权重$W$和激活$a$，反向传播训练时候的误差$e$和梯度$g$。作者们把“误差”的定义扩展到了多层上：对于每个卷积层或者是全连接层，误差$e$指的是激活$a$的梯度。而梯度$g$专门指的是权重$W$的梯度累加和。考虑在一个前向传播的网络中，第$i$个层：

$$ e^i = \dfrac{\partial L}{\partial a^i}, g^i = \dfrac{\partial L}{\partial W^i} $$

上面式子中的$L$是损失函数。作者们把$e$和$g$给拆开了，而在通常的表示中，它们两个通常都混在一起。权重的梯度$g$和激活的梯度$e$在每个层的数据流中走的是不同的路线，整体上可以看成是一个MAC操作节点。

在第$i$层的前向传播过程中，假设权重的累加和计算都是用$k_G$个比特的整数进行的，那么需要做出来一个量化函数$Q_W(.)$，把高精度的权重映射到$k_W$个比特。例如，把$[-0.9, 0.1, 0.7]$映射成$[-1, 0, 1]$。尽管权重是用高精度（例如32比特浮点型）进行累加的，但是在训练之后，这些映射在特定硬件上的执行非常省内存。激活用$Q_A(.)$量化到$k_A$个比特，用来把MAC引入的比特位宽进行对齐。在前人的工作中，权重和激活甚至离散化到了二值，然后MAC降级到逻辑运算和bit-count操作，这样的效率会非常高。

在第$i$层的反向传播过程中，激活的梯度，以及权重，是用MAC计算的导数，通常认为至少需要16比特浮点数的精度。在作者提供的计算数据流图中，MAC的两个输入分别是输入（$k_A$个比特）和权重（$k_W$个比特）。如果看成是带符号整型，那么输出的比特位数是$[k_A+k_W-1]$。对误差$e$来说，也有类似的“拓宽”位数的情况出现。考虑到训练的时候只用低比特位宽的整数，作者们另外提出了两个函数$Q_E(.)$和$Q_G(.)$，用来限制$e$和$g$的位宽，分别到$k_E$个比特和$k_G$个比特。整体来说，在MAC操作中，在推理和反向传播阶段，有$Q_W(.)$、$Q_A(.)$、$Q_G(.)$、$Q_E(.)$一共4个量化操作符。

### 3.1 基于移位的线性映射和随机取整

在WAGE量化中，为了简单，作者们使用了一个线性映射，映射到$k$个比特的整数。即，一个连续的并且无界的值，会映射成：

$$ \sigma(k) = 2^{1-k}, k \in \mathbb{N}_+ $$

因此，最基本的量化函数如下，它把一个浮点型的数值$x$映射成$k$比特的带符号整型变量：

$$ Q(x,k) = Clip \left\{ \sigma(k) \cdot round \left[ \dfrac{x}{\sigma(k)} \right], -1+\sigma(k), 1-\sigma(k) \right\} $$

这里的round函数把连续值映射到最接近的离散状态。Clip函数是饱和函数（saturation），把无界的值映射到$[-1+\sigma, 1-\sigma]$范围内。注意负的最大值$-1$被移除，为了保证对称性。例如，$Q(x,2)$把$\\{-1, 0.2, 0.6\\}$映射到$\\{-0.5, 0, 0.5\\}$。上面的公式（3），只是用来在GPU等浮点计算硬件上面进行仿真（simulation）用的。如果是在定点设备上用，量化和饱和都自动完成。

在对某些操作数（例如，误差）进行线性映射之前，需要先介绍一个统一的（monolithic）缩放因子，用来把数值的权重分布给“平移”到一个适合幅度范围内，否则的话这些值会都被公式（3）给饱和掉或者清除掉。

缩放因子用下面的Shift函数计算，算完了之后会用来做除法：

$$ Shift(x) = 2^{round(\log_2{x})} $$

最后，再用随机取整，用来替换训练中对梯度累加的各种小的和实数值的更新。后面的节3.3.4会详细介绍$Q_G(.)$操作符：高比特宽度的梯度，会用一个16比特随机数生成器，限制在$k_G$个比特的整数中。

### 3.2 权重初始化

在一些前人的工作中，权重会用sgn函数（符号函数）处理成二值，或者用训练时候获得的阈值参数处理成三值。BNN在不用batch normalization的时候无法收敛，因为对于一个典型的DNN来说，权重值是$\pm 1$是相对比较大的值了。batch normalization不但有效的解决了梯度爆炸（explode）和梯度消失（vanish）问题，而且减轻了对适合的初始化的需求。尽管如此，如果没有浮点运算器（floating point unit，FPU）的话，对每个层的输出进行归一化然后计算梯度，会很复杂。此外，计算每个小批量数据的滑动平均（moving average）会需要额外的内存开销。BNN里面用了一个基于平移的对batch normalization的变种，但是也很难把所有的元素（element）都转换成定点表示。结果就是，在这篇文章的工作中，必须仔细设计权重的初始化，因为batch normalization被简化成为一个常数的缩放层（scaling layer）。

基于2015年MSRA的方法，稍作修改，得到了下面的公式，用来初始化：

$$ W \sim U(-L, +L), L = \max \left\{ \sqrt{6/n_{in}}, L_{min} \right\}, L_{min} = \beta \sigma $$

在上式中，$n_{in}$是这个层的输入节点数。在MSRA论文中原始的限制$\sqrt{6/n_{in}}$用来保证同一个层的输入和输出有相同的方差。增加了一个$L_{min}$的限制，它是一个最小值，也就是均匀分布（uniform distribution）$U$能触及到的最小值。$\beta$是一个常数，比$1$稍大，用来在最小步长$\sigma$和最大值$L$中间产生重叠（overlap）。在使用$k_W$个比特的线性映射中，如果权重$W$用原始的限制进行量化，那么当设置的比特位宽$k_W$设置的太小的话，所有的向量都会变成$0$。另外，如果输入节点的个数$n_{in}$足够宽，也会都刷成$0$，初始化的权重根本到不了用定点整数表示的最小步长$\sigma$。所以$L_{min}$保证了在随机初始化的时候，权重能超过$\sigma$，并且在使用$Q_W(.)$进行量化之后，能得到非零值。

### 3.3 量化细节

#### 3.3.1 权重$Q_W(.)$

前面提到的修改之后的初始化方法（公式5），会把权重进行一个整体的增强，并且保证它们的正确分布。然后权重$W$直接用公式3进行量化：

$$ W_q = Q_W(W) = Q(W, k_W) $$

注意权重的方差对于原始的限制来说也被缩放了，这个会导致网络的输出爆炸。为了减轻这种增强效应，XNOR-Net提出了一种方法，每个filter用不同的缩放系数，并且用全精度（full precision）来计算这个连续值。本文的作者们考虑到当前的整数实现，提出了这样的一种方法，每个层一个系数，并且是基于移位的，记这个系数是$\alpha$，来减弱前面提到的增强效应：

$$ \alpha = \max \left\{ Shift(L_{min}/L), 1 \right\} $$

这里面$\alpha$是根据网络结构决定的，每个层都不一样的一个预先定义好的常量。修改之后的初始化方案，以及这个减弱因子$\alpha$，一起把浮点型的权重近似到它们的整数表示中。此外在激活之后，$\alpha$还起着用来保证权重的精度是$k_W$个比特的作用。

#### 3.3.2 激活$Q_A(.)$

之前已经说过，在经过了MAC之后，操作数的比特宽度会变大。然后一个典型的卷积神经网络（CNN）通常会后面接池化（pooling）、归一化和激活。作者们不用均值池化（average pooling），因为取均值的操作会提高对精度的需求。另外，作者们假设了每个隐层的每个batch的输出都近似有0均值，因此batch normalization退化成一个缩放层，原先的可以训练的并且每个batch都不同的那个缩放因子，被上面公式7提到的$\alpha$所替代。如果激活是用$k_A$个比特出现，那么整体对激活的量化可以用下面的式子来表示：

$$ a_q = Q_A(a) = Q(a/\alpha, k_A) $$

#### 3.3.3 误差$Q_E(.)$

在训练中，基于链式法则（chain rule），误差$e$会一层一层计算。尽管反向传播的计算图和推理阶段的很像，但是它的输入是$L$的梯度，相比网络的真实输入要小很多。更重要的是，计算出来的误差是没有范围的，并且可能有显著更大的范围，比激活的范围要大，例如$[10^{-9}, 10^{-4}]$。DoReFa-Net首先对误差$e$应用了一个仿射变换（affine transform），映射到$[-1, 1]$，然后在量化之后再反转回来。因此，量化之后的$e$仍然可以使用32位浮点型和离散状态，并且通常是很小的值。

尽管如此，实验显示，误差的“方向”和“阶数”相比，方向更为重要，能指导前一层收敛，因此在DoReFa-Net中的那个量化之后的反变换并不需要。这种只保留方向的做法提示了本文的作者们，用整数传递误差，不过需要首先把误差的分布缩放到$[-\sqrt{2}, +\sqrt{2}]$，方法是先除以一个因子，然后再用$Q(e,k_E)$进行量化：

$$ e_q = Q_E(e) = Q( e / Shift(\max\{|e|\}), k_E) $$

这里的$\max\\{\|e\|\\}$提取了每个层的误差$e$的所有元素中的绝对值的最大值，卷积的话需要考虑所有的通道（channel），小批量训练的话需要考虑多条样本。这种对误差的量化，会扔掉小于$\sigma$的大量数据，会在后面讨论对准确率的影响。

#### 3.3.4 梯度Q_G(.)

由于在移位之后只保留了误差的相对值，因此，从MAC出来的梯度更新$g$（用反向误差$e$和前向激活$a$计算出来的）也就相当于移位了。首先要对梯度$g$进行重新缩放，用另外一个缩放因子，并且还要引入同样基于移位的学习率$\eta$：

$$ g_s = \eta \cdot g / Shift(\max\{|g|\}) $$

上式中的$\eta$是$2$的整数次幂。这里移位之后的梯度$g_s$，表示了更新权重的最小步数和方向。如果权重用$k_G$个比特来保存，那么修改它的最小步长，对于整数来说是$\pm 1$，对于浮点数来说是$\pm  \sigma(k_G)$。

这种对于学习率$\eta$的实现，和普通的那种基于32位浮点数的实现有很大不同。在WAGE里面，这里只保留了权重的改变方向，而步长则是最小步长$\sigma$的整数倍。在刚开始训练的时候，移位之后的梯度$g_s$可能比$1$大（如果$\eta$是$2$或者更大），以加速训练。而在训练的后半段（通常会用学习率衰减等等策略），则可能小于$0.5$。像原文中图2展示的那样，为了把小梯度的求和计算给替换掉，特别是在前面两种情况的后者，作者们把$g_s$给拆开，拆成整数部分和小数部分，然后用了一个16比特随机数生成器，来限制$g_s$到$k_G$个比特：

$$ \Delta W = Q_G(g) = \sigma(k_G) \cdot sgn(g_s) \cdot \left\{ \lfloor |g_s| \rfloor + Bernoulli(|g_s|- \lfloor |g_s| \rfloor ) \right\} $$

上式中的Bernoulli随机把小数部分采样到$0$或者$1$。当$k_G$设置合适时，对梯度的量化会限制最小步长，进而可能避免了局部最小值（local minimum）和过拟合。更多的，当$\eta$不大于$1$的时候，梯度会变成三元值，这就可以降低分布式训练的通信开销。最后，权重$W$在用了离散增量$\Delta W$进行更新后，可能需要超过用$k_G$个比特整数的表示范围$[-1+\sigma, 1-\sigma]$。因此Clip函数是不可缺少的，用来饱和，保证权重的累加和里面只有$2^{k_G-1}-1$个状态。那么在第$t$轮的时候，更新公式是：

$$ W_{t+1} = Clip \left\{ W_t - \Delta W_t, -1 + \sigma(k_G), 1 - \sigma(k_G) \right\} $$

### 3.4 其他杂项

上面说过的各种东西，已经展示出了作者们提出的对权重、激活、梯度和误差的量化方法。可以根据文中的算法1来看详细的计算过程。在整个全部只用整数进行的训练过程中，还剩余一些需要指出的问题：

梯度下降需要的优化器，例如Momentum，RMSProp和Adam这几个，内部都含有至少一份梯度更新量$\Delta W$的副本，或者是它们的滑动平均。因此在训练的时候会占用双份的内存消耗，这和使用更大的$k_G$部分等价。由于权重更新量$\Delta W$已经量化成了$\sigma$的整数倍并且被$\eta$缩放，作者们用的是纯小批量随机梯度下降（mini-batch SGD），没有用任何的动量或者是自适应学习率，用来显示减少存储量的需求的可能性。

尽管L2正则化在很多大规模DNN中（经常过拟合）很有帮助，WAGE在公式（3）中移除了比较小的值，并且在公式（11）中引入了随机性，也作为某种的正则化，并且在实验中也获得了有竞争力的准确率。所以作者们保留了L2的权重衰减，以及dropout作为补充的正则化方法。

Softmax层以及交叉熵（cross-entropy）判别在分类问题中有大量的应用，但是计算$e^x$在低比特位宽的线性映射场景下是非常难的。对于分类的数目较小的任务，作者们不用Softmax层，而是改用均方误差（mean-square-error），但是又扔掉了求均值的操作，变成了平方误差和（sum-square-error，SSE），因为在公式（9）中，移位之后的误差都会得到相同的值。

