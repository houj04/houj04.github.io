---
layout: post
title: "Distilling the Knowledge in a Neural Network"
categories: mypost
---

# Distilling the Knowledge in a Neural Network

## 论文介绍

* 文章标题：Distilling the Knowledge in a Neural Network
* 作者：Geoffrey Hinton, Oriol Vinyals, Jeff Dean
* 发表于：NIPS 2014 Deep Learning Workshop
* 参考：https://arxiv.org/abs/1503.02531

## 摘要

一个非常简单的，可以提升几乎任何机器学习算法（machine learning
algorithm）的表现（performance）的方法是：在同一份数据上面训练多个不同的模型，然后将它们的预测结果进行平均（average）。不幸的是，使用一整套（ensemble）模型进行预测是非常笨重复杂（cumbersome）的，并且，当部署到大量用户的时候，可能计算上开销很大（computationally expensive）。特别是单个模型是大的神经网络的时候。前人Caruana和合作者们已经指出了，可以把一系列模型中的知识压缩到单个模型中，部署的难度会显著降低。作者们把这个方法进行了进一步的开发，使用了一种不同的压缩技术。作者们在MNIST数据集上获得了一些令人惊奇的结果，并且展示了可以大幅度提升在商业系统中大量使用的声学模型（acoustic model），通过把一系列模型的知识进行蒸馏（distilling）到一个单模型中。作者们同时提出了一种新的把多个完整模型（full model）进行整合的方法，得到的专家模型（specialist model）学习到区分出能让原始完整模型混淆的细粒度的类别（fine-grained classes）。和专家的混合（a mixture of experts）不同，这些专家模型可以快速并且并行训练。

## 1 简介

多种昆虫都有一种幼虫形态，被优化于从环境中吸取能量和养分。同时还有一种完全不同的成虫形态，是为了移动和繁殖而进行优化的，这和幼虫时期的需求非常不同。在大规模机器学习（large-scale machine learning）中，尽管在训练阶段和部署阶段，它们的需求不同，我们也还是会使用很相似（very similar）的模型：例如语音和目标识别（object recognition）任务，训练阶段需要从很大量、高度重复的数据中抽取出结构来，但是不需要实时进行处理（operate in real time），并且可以使用非常大的计算量。但是在部署到给大量用户的时候，对延迟（latency）和计算资源消耗上就有更多严格的（stringent）要求。对昆虫的这种分析，提示我们应该训练非常复杂的模型，如果能够使得从数据中提取结构变得更简单。这种复杂的模型可以是一系列分别训练的模型的组合，也可以是一个单独的非常大的模型，用很强的正则化（very strong regularizer），例如dropout。一旦这个复杂的模型训练好了，我们就可以用另外一种训练，这里称之为“蒸馏（distillation）”，把复杂模型中的知识给迁移（transfer）到一个小模型上，而小模型是更适合用来部署的。这种策略的已经被前人Caruana及其同伴进行了领先的实现。在他们的重要的论文中，他们展示了令人信服的结论：一大堆模型的混合得到的知识，可以被迁移到一个单独的小模型上。

上面的这个看起来很有希望的做法，可能被这样的想法阻挡我们进行进一步的研究：我们尝试去在一个已经训练好的模型中，包括训练出来的参数值（parameter values），从这里面尝试出识别（identify）出具体的知识来，于是这就变得非常难，无法对模型进行形式上的变换同时还能够保留原始的知识。另一个对“知识”的更抽象（abstract）的看法，抛弃了任何的具体的实例化（instantiation）：它就是一种训练出来的映射（mapping），从输入的向量（vector）到输出向量。对于一个用来区分很多个种类的复杂模型来说，通常的训练目标是，最大化（maximize）正确答案的平均对数概率（average log probability）。但是这种训练方法有个副作用：学出来的模型会把概率分配到所有的非正确答案上面，有些概率非常小，有些概率比其它的明显要大。这种在非正确答案上面分配的概率的相对大小，可以给我们以大量提示，告诉我们这个复杂模型是如何趋向泛化（generalize）的。例如，一张宝马（BMW）的图片，可能有一个很小的概率被识别成垃圾车（garbage truck），但是这种错误的概率，比识别成胡萝卜（carrot）的概率仍然要高很多。

通常大家都认为，在训练中的目标函数（objective function）应该能够反映用户的真正的目标，越接近越好。尽管如此，当真的目标是在新数据上有好的泛化能力的时候，模型还是在训练集上面进行优化的。显然应该是让模型的泛化能力更好，但是这需要对如何泛化这件事情需要很多信息，然后这种信息通常都是拿不到的。当我们在把一个大模型进行知识蒸馏，希望获得一个小模型的时候，我们可以训练这个小模型使得它的泛化能力和之前的大模型一样。如果复杂的模型能够很好泛化，比如，它是一系列不同模型的平均，那么按照这种方式训练出来的小模型，也会表现更好（相对于用通常方法在训练集上直接训练小模型来说）。

把泛化能力从复杂模型迁移到小模型，一个明显的做法是，把大模型输出的分类概率看成是训练小模型时的“软目标（soft targets）”。在这个迁移阶段，我们可以用同样的训练集，或者一个专门的“迁移”数据集。当复杂的模型是由一系列简单模型组合而成的话，我们可以用算术平均值（arithmetic
 mean）或者几何平均值（geometric mean），把这一大堆模型分别的输出进行处理，来当作软目标。当这些软目标具有高的熵（entropy）的时候，对于单个训练样本（training case），相比于原始的硬目标（hard targets）可以提供明显更多的信息，然后梯度中的低方差（less variance in the gradient）可以提供训练样本之间的信息。因此相对于原始的复杂模型来说，小模型可以通常在显著更少的数据上进行训练，并且使用显著更高的学习率（learning rate）。

对于MNIST这样的任务来说，复杂模型通常总是能以一个非常高的置信度（confidence）给出正确的答案，学到的这个函数中还有很大一部分信息，是通过软目标的那些概率很小的类的概率值的比例，而存在的。举例来说，某一个“2”可能以$10^{-6}$的概率被识别成“3”，以$10^{-9}$的概率被识别成“7”，然后另外一个“2”可能就反过来了。这些信息是非常有用的，因为它描述了数据之间的丰富的相似性结构（例如，描述了什么样的“2”会更像“3”，以及什么样的“2”会更像“7”）。但是这种信息，对于迁移时候用的交叉熵损失函数（cross-entropy cost function）来说，影响非常非常少，因为这些概率和0太接近了。Caruana等人用来避免（circumvent）这个问题的做法是，直接使用logits（即，送到最后的softmax层的原始输入），而不是softmax之后输出的概率，来作为小模型的训练的目标。然后他们最小化的是复杂模型输出的logits和小模型输出的logits的平方误差（squared difference）。本文作者提出的更通用的做法，称为“蒸馏物（distillation）”，是提升最后的那个softmax的温度（temperature）直到那个复杂模型输出了一个合适的软目标。然后在训练小模型的时候，使用相同的高温度，来匹配前面输出的软目标。后面将会看到，“匹配复杂模型输出的logits”实际上是蒸馏物的一个特例。

用来训练小模型的迁移数据集（transfer set），可以完全使用未标注的数据（unlabeled data），也可以用原始的训练集。本文作者们发现使用原始的数据集工作得不错，特别是如果在目标函数上增加一个小的项（term），鼓励小模型预测真实的目标，同时匹配复杂模型输出的软目标。通常情况下，小模型是不能完全匹配软目标的，但是朝着正确答案的方向是有帮助的。

## 2 蒸馏

神经网络通常会使用一个叫做“softmax”的输出层（output layer），把各个类的原始的logit，$z_i$，转换成概率$q_i$，做法是把$z_i$和其它的logit相比较。

$$q_i = \dfrac{\exp(z_i/T)}{\Sigma_j \exp(z_j/T)}$$

上式中的$T$是温度（temperature），通常设置为1。把$T$设置成更高的值，就会在各个类上产生更加软（softer）的概率分布。

最简单的形式的蒸馏，使用一个迁移数据集，对迁移数据集中的每一条样本，在复杂模型中，把softmax层的温度设置为一个较高的值，然后进行计算，得到一个软的目标分布，然后用这样的结果去训练小模型，来实现知识的蒸馏。在训练蒸馏之后的模型的时候，使用相同的高温度，但是训练之后，温度设置为1。

如果对迁移数据集中的全部样本，或者部分样本，知道正确的标注，那么上面的方法可以被极大提升：同时训练蒸馏后的模型，让它能够产生正确的标注。实现这个的方法之一是，用正确的标注来修改软目标。但是本文作者发现有一个更好的办法：直接把两个不同的目标函数的值，简单进行加权平均（weighted average）。第一个目标函数是软目标的交叉熵，交叉熵在计算的时候使用和蒸馏模型的softmax层（在复杂模型中生成软目标也一样）中的相同的高温。第二个目标函数是用正确标注的交叉熵。这里会用蒸馏后的模型的相同的logit，送到softmax的时候的温度设置为1。作者们发现把第二个目标函数的权重设置更低一些，通常会获得更好的结果。由于计算软目标的时候，梯度的大小（magnitudes of the gradients）会变成正常的$1/T^2$，所以当同时使用硬目标和软目标的时候，一个重要的地方是，一定要用$T^2$去乘一下。这就保证了当实验不同温度的蒸馏模型时候，硬目标和软目标的相对贡献整体上还是保持不变的。

### 2.1 对logits的匹配是蒸馏的一个特例

迁移数据集中的每一条样本都贡献了一个交叉熵的梯度，$dC/dz_i$，对每一个logit，然后$z_i$是蒸馏后的模型。如果复杂模型有一个logits是$v_i$，产生了软目标概率$p_i$，然后迁移训练是在温度$T$下进行的，那么梯度就是：

$$ \dfrac{\partial C}{\partial z_i} = \dfrac{1}{T} (q_i-p_i) = \dfrac{1}{T} \left(\dfrac{e^{z_i/T}}{\sum_j e^{z_j/T}} - \dfrac{e^{v_i/T}}{\sum_j e^{v_j/T}} \right)$$

如果相对于logits的数量级，温度很大，那么就可以进行近似：

$$ \dfrac{\partial C}{\partial z_i} \approx \dfrac{1}{T} \left(\dfrac{1+z_i/T}{N+\sum_jz_j/T} - \dfrac{1+v_i/T}{N+\sum_jv_j/T}\right)$$

如果假设对于每一条迁移样本，logits都有零均值（zero-meaned），那么就有$\sum_jz_j = \sum_jv_j =0$，所以上面的式子3就可以进一步简化：

$$ \dfrac{\partial C}{\partial z_i} \approx \dfrac{1}{NT^2} (z_i-v_i)$$

所以在高温度的限制下，蒸馏就相当于最小化$1/2(z_i-v_i)^2$（在logits是零均值的前提下，分别对每个迁移样本）。在低温的情况下，蒸馏对那些比平均值负得很多（much more negative than the average）的logits的关注度要低。这是有潜在优势的，因为这些logit几乎是完全没有被目标函数限制（unconstrained）的（在训练复杂模型的时候）。因此它们很可能非常混乱（noisy）。另一方面，这些负得很厉害的logits，可能传送了复杂模型获得的一些有用的信息。至于上面说的这些影响，哪个会起到主导作用，是一个经验上（empirical）的问题。本文作者发现，当蒸馏后的模型特别小，以致于无法获得复杂模型的全部知识的时候，中等大小的温度最好，进而强烈指出忽略较大的负logits是有用的。

## 3 在MNIST上的初步实验

为了看到蒸馏工作的如何，作者们训练了一个单独的大的神经网络，有两个隐藏层，每个包含有1200个使用了rectified linear的隐层节点。使用了全部的60000条训练样本。这个网络用了dropout和weight-constraints以进行强正则化。dropout可以看成是一种训练了指数方式（exponentially）更大的共享权重（share weights）的一堆模型。此外，输入图像每个方向最多抖动（jitter）两个像素（pixels）。这个网络获得了67个测试误差，而另一个稍小一点的网络，中间两个隐藏层是800个节点，不使用正则化，获得了146个测试误差。但是如果这个小网络单独进行正则化，方法是加上了一个用来匹配大模型在温度为20时候的输出的软目标，它可以获得74个测试误差。这就说明了软目标可以将一大部分知识传送给蒸馏后的模型，包括从转换后的训练集中学到的如何泛化的知识。甚至这个迁移数据集并不包括任何的转换。

当这个蒸馏后的网络，两个隐藏层有300或者更多个元素的时候，所有超过8的温度都给出了相近似的结果。但是如果每个层的大小激进地（radically）降低到30，温度在2.5到4这个范围内，会显著比更高或者更低的温度要表现更好。

作者们继续尝试了在迁移数据集中去掉了所有的数字3。那么从蒸馏后的模型角度看过去，3是一个神秘的从来没有见过的数字。然而这个蒸馏后的模型，只造成了206个错误，其中有133个是测试集中的1010个数字3。这些错误的多数，实际上是因为对于3这个类别，学到的偏置（bias）太低了。如果这个偏置调整到3.5（这也提升了测试集的整体指标），蒸馏后的模型会给出109个错误，其中14个是关于数字3的。所以在使用了正确的偏置时，这个蒸馏后的模型在3这个数字上面获得了98.6%的正确率，而在训练中其实它根本没有见过数字3。如果迁移数据集里面只包括7和8两种数字，蒸馏后的模型会造成47.3%的错误，但是如果把7和8的偏置降低到7.6来优化测试集的整体性能，错误率会下降到13.2%。

## 4 在语音识别上的实验

在这一节，作者们将研究在自动语音识别（Automatic Speech Recognition，ASR）中，深度神经网络（Deep Neural Network，DNN）在声学模型（acoustic model）上的效果。作者们将展示，在这篇论文中提出的蒸馏策略，可以将一系列的模型进行蒸馏，整合到一个单模型中，获得期望的效果，同时比一个直接从同样的训练样本开始训练，大小相同的模型，效果显著要好（significantly better）。

现代的（state-of-the-art）语音识别系统中，使用DNN来映射一个短时的带上下文的特征。这个特征是从波形（waveform）信息的隐马尔可夫模型（Hidden Markov Model，HMM）的离散状态（discrete states）的概率分布中获得的。更准确地说，DNN对每一个时刻的三音素状态（tri-phone states）的概率分布进行描述，然后一个解码器（decoder）会找到一个穿过所有HMM状态的概率最高的路径，然后产出一系列标注（transcription）（可能还需要一个语言模型（language model））。

尽管可以（并且推荐）这样训练DNN：解码器以及语言模型直接参与计算，将所有的可能的路径进行边缘（marginalize），但是更通常的做法是，用DNN对每一帧（frame-by-frame）进行分类，（局部（locally））最小化交叉熵（输入是：网络的预测结果，以及对标注数据序列（ground truth sequence）的强制对齐（forced alignment）之后得到的label）。

$$ \theta = \arg \max _{\theta'} P(h_t | s_t; \theta') $$

上式中，$\theta$是声学模型$P$的参数，将$t$时刻的声学观察值$s_t$映射到一个概率$P(h_t \| s_t; \theta')$，即“正确的”HMM状态$h_t$。这个状态是用正确的标注出来的词序列进行强制对齐而得到的。然后整个模型使用分布式随机梯度下降（distributed stochastic gradient descent）的方法进行训练。

作者们使用这样的网络结构：有8个隐藏层，每一个隐藏层有2560个rectified linear单元，最后有一个softmax层，有14000个label（对应HMM的目标$h_t$）。输入是26帧，使用了40个梅尔滤波器系数（Mel-scaled filterbank coefficients），每帧之间的步进（advance per frame）是10毫秒，预测第21帧的HMM状态。总共的参数个数大约是85M个。在安卓的语音搜索上面，这还是一个稍有过时（a slightly outdated version）的声学模型，应该可以认为是一个很强的基线。训练这个DNN声学模型，作者们使用了大约2000小时的英语口语数据，大约是700M条训练样本。这个系统在开发集（development set）上获得了58.9%的帧准确率，以及10.9%的词错误率（Word Error Rate，WER）。

### 4.1 结果

作者们训练了10个独立的模型，来预测$P(h_t \| s_t;\theta)$，用和基线系统完全一致的结果以及训练流程。这些模型使用了随机初始化（randomly initialized），用了不同的初始化参数，作者们认为这样可以在这批模型中间建立了足够的多样性（diversity），进而让平均之后的预测结果可以显著比单独的模型要好（significantly outperform）。作者们也尝试过，让每个模型看到的数据不同，以增加模型的多样性，但是作者们发现这样做对结果并没有多大的影响，所以还是选用了较为简单的方法。在蒸馏的时候，作者们尝试了用1、2、5、10作为温度，交叉熵中间的硬目标的相对权重用的是0.5。

论文中的表格显示，确实作者们的蒸馏方法，可以从训练集中提取出更多的有用信息，相比较直接用硬目标来训练单个模型来说。通过使用10个模型的综合，以得到的帧的分类准确率的提升，这部分的提升有80%都迁移到了蒸馏之后的模型上，和作者之前在MNIST上所看到的提升是接近的。在最终目标的词错误率（在一个23k个词的测试集上）上，这个综合提供了一个小小的提升，因为目标函数不完全匹配，但是再一次，在词错误率上的提升迁移到了蒸馏之后的模型上。

作者们近期注意到了一些相关的工作，通过一个已经训练好的较大的模型的分类输出，用小的声学模型去匹配这样的输出。但是，他们的做蒸馏的时候温度是1，用的是很大的未标注的数据集，并且他们的最好的蒸馏模型也只是把小模型上的错误率降低了28%（相对于大模型和小模型之间的错误率差距来说，在大模型和小模型都是用硬标注来训练的时候）。

## 5 在非常大的数据集上，训练一批专家模型

训练一大批模型是一种充分利用并行计算（parallel computation）的非常简单的方法，通常关于这种思想的异议（objection）来自于这种综合模型在预测阶段需要太多的算力，而这个问题却可以被蒸馏解决。尽管如此，这里还有另外一个问题：如果这些单独的模型都是很大的神经网络，并且训练数据集很大，那么在训练期间需要的训练时间也是非常多的（excessive），即使能够很容易进行并行化。

在本节中，本文作者给出这样一个例子，来展示如何训练一系列的专家模型，每个模型专注于一个不同的、易混淆的（confusable）类别的子集，这样就可以减少所需要的总的计算量。专家模型的主要问题在于，这些模型专注于细粒度的区分，进而非常容易过拟合（overfit）。作者将会描述这种类型的过拟合，可以用软目标的方法来进行阻止。

### 5.1 JFT数据集

JFT是谷歌（Google）公司内部使用的数据集，里面有1亿张带标注的图片，一共有15000个标注。本文作者在进行本文工作的时候，谷歌公司的在JFT数据集上的基线模型是一个深度卷积神经网络（deep convolutional neural network），已经训练了大约6个月的时间，使用异步随机梯度下降法（asynchronous stochastic gradient descent），用了很多个核（core）。这个训练包含了两种类型的并行化。第一，神经网络有很多的副本（replicas），在不同的核上运行，处理训练集里面不同的小批量数据（mini-batches）。每个副本计算当前的小批量数据，计算平均梯度，发送梯度到分块的（sharded）参数服务器（parameter server），参数服务器会返回这些参数的新的值。这些新的值反映了从上次参数服务器发送参数以来，参数服务器收到的所有的梯度。第二，每个副本都是分布在多个核上的，把神经元的不同子集分配到每个核上面。综合训练也可以看成是第三种并行化，可以包在前两种上面，但是只能在拥有非常多的核的时候才行。等好几年才能获得一个综合的模型不是一个好主意，所以需要一个明显更快的方法来优化基线模型。

### 5.2 专家模型

当类别数非常多的时候，关于复杂的模型，下面的这个思路是合理的：模型由一系列模型组成，其中一个是通才（generalist）模型，用全部数据训练，剩下的都是“专家（specialist）”模型，每一个都是用数据集中找到的一些非常容易混淆的类别的子集的数据组成（例如，蘑菇的不同种类）。对于这种专家模型来说，最后一层的softmax可以做的明显小很多，因为可以把那些它不需要考虑的类别全都扔到一个“垃圾桶（dustbin）”类里面。

为了减少过拟合，并且共享低层的特征检测器，每个专家模型都是用通用模型的权重来进行初始化。然后它的权重会在训练过程中被细微修改：其中一半的训练样本来自于它本身的特殊子集，另一半是从训练集的剩余部分里面随机进行采样得到。在训练之后，可以对这个有偏的训练集进行修正，方法是把垃圾桶类别的logit进行增加，根据专家类别过度采样的比例的对数。

### 5.3 将类别指派给专家

为了能够把目标的类别进行分组以指派给专家，作者们决定盯着那个完整的神经网络经常混淆的那些类别。尽管可以计算混淆矩阵（confusion matrix）然后用它来找到这样的聚类（cluster），作者们使用了一个更加简单的方法，不需要用真实的标注来构建聚类。

具体来说，作者们在通用模型的输出的协方差矩阵（covariance matrix）上面运行一个聚类算法，这样一系列类别$S^m$（经常预测在一起）就当作专家模型$m$的训练目标。作者们用了一个在线版本的k-means算法，对协方差矩阵的列进行计算，然后得到了一些合理的聚类。作者们尝试了多种聚类算法，也得到了相似的结论。

### 5.4 在一系列专家模型上进行预测

在讨论专家模型蒸馏的时候具体发生了什么之前，作者们想看一下带专家模型的聚合模型表现如何。除了一系列专家模型以外，一直还有一个通用模型，因此可以用来处理那些没有专家模型来处理的类别，以及决定使用哪个专家。给定一个输入图片$x$，做一个找最接近类别（top-one classification）的操作，分两个步骤：

第一步，对每一个训练样本，根据通用模型找到$n$个最可能的类别。把找到的这个类别的集合叫做$k$。在作者们的实验中，用的是$n=1$。

第二步，从所有的专家模型中找到这样的$m$，$m$的易混淆的特殊子集，$S^m$，和$k$有一个非空的交集（non-empty intersection），把这个叫做专家的活动集（active set），记作$A_k$。（注意这个集合可能是空的）。然后在所有的类别上，找到完整的概率分布$q$，使得能够最小化下面的式子：

$$ KL(p^g,q) + \sum_{m \in A_k} KL(p^m, q)$$

这里KL表示KL散度（KL divergence），$p^m$和$p^g$表示专家模型或者是通用的完整模型的概率分布。分布$p^m$是一个在所有的专家类别$m$上的分布，加上一个单独的垃圾桶类别，所以用完整的$q$分布计算KL散度的时候，作者们把完整的$q$分布里面，指向$m$的垃圾桶中的那些类别里面的概率，全都加在一起。

上面的公式没有通用的闭式解（closed form solution），尽管当所有的模型都对每一个类都产出一个单独的模型，要么是算术平均值，要么是几何平均值，取决于用$KL(p,q)$还是$KL(q,p)$。把$q$进行参数化$q=softmax(z), T=1$，然后用梯度下降法来优化$z$的logits。注意这个优化需要对每一个图片都进行。

### 5.5 结果

从训练好的完整神经网络作为基线，专家模型训练速度非常快（只需要几天，而不是JFT的几个星期）。并且，所有的专家模型都是完全独立进行训练的。论文中的表3展示了基线系统的绝对测试准确率，以及基线系统与专家模型合并的结果。使用了61个专家模型，在整体的测试准确率上有一个4.4%的相对提升。作者们还报告了条件下的测试准确率，也就是只考虑属于专家类型的类别的样本，然后限制预测结果到类别的子集中。

对于作者们的JFT专家实验，训练了61个专家模型，每一个有300个类别，再加上垃圾桶类别。由于每个专家的类别集合不是不相交的（not disjoint），经常会出现“某个特殊的类别，被多个专家所使用”的现象。论文中的表4展示了测试集合的样本数量，使用了专家模型之后，在第一位就分类正确的样本数量的变化，JFT数据集被多少个专家覆盖的top-1准确率的相对百分比提升。作者们觉得很受鼓舞，从整体趋势上来看，当有更多专家覆盖某一个特定类别的时候，准确率提升会越来越高。因为训练独立的专家模型是非常容易并行化的。

## 6 用软目标作为正则化

作者们关于使用软目标而不是硬目标的一个重要说明的是，有大量的有用信息是可以在软目标中被承载，而在单一的硬目标中是无法记录的。在这一节，作者们展示这是一个很大的影响（用少很多的数据），来拟合这个85M个参数的基线语音识别系统。表格5展示了，只有3%的数据（大约20M样本）时，用硬目标来训练基线模型的时候会遇到很严重的过拟合（severe overfitting）（作者们使用了提前终止（early stopping），当准确率达到44.5%之后快速下跌）。而同样的模型，使用软目标进行训练的时候，就能够还原在完整训练集中几乎所有的信息（2%）。更值得注意的是，不需要进行提前终止：使用了软目标的系统，简单“收敛（converge）”到了57%。这说明软目标是一个非常有效的方法，把全量数据上训练出来的规则性（regularities）沟通（communicating）给另一个模型。

### 6.1 用软目标来防止专家模型过拟合

作者们在JFT数据集上的实验中使用的专家模型，会把所有的非专家类别都放到一个单独的垃圾桶类里面。如果允许专家模型使用一个完整的softmax，在所有的类别上，那么可能有一个明显更好的方法来阻止它们过拟合，而不是靠提前终止。一个专家模型是用这样的数据训练的：在数据集中有一些特别的类别，数据特别密集。这说明它的训练集中的有效大小（effective size）是非常小的，然后就有一种强的趋势去对它的专家类别过拟合。这个问题不能这样的方法：把专家设置成明显更小，因为这样的话在训练模型的时候，就会丢失在所有的非专家类别中的非常有帮助的迁移效果。

作者们的那个只用了3%语音数据训练集的实验强烈说明，如果一个专家是用通用模型的权重进行初始化的，那么可以让它保留（retain）它的关于非特殊类别的全部知识：在用硬目标训练之上，追加对这些非特殊类别的软目标训练。这些软目标可以用通用模型来生成。作者们目前正在研究这种做法。

## 8 讨论

作者们已经展示了，用蒸馏的方法，可以很好地将知识进行迁移。可以从一系列模型，或者从一个很大的高度正则化的模型，迁移到一个小的、蒸馏之后的模型上。在MNIST数据集上的蒸馏工作效果显著，甚至在训练蒸馏后的模型的时候完全缺失一个或者多个类别的样本的时候。深度声学模型方面，对于在安卓语音搜索上面的版本，作者们展示了几乎所有对深度神经网络进行的综合训练上的提升，都可以迁移到一个单个的神经网络中，并且明显更易于部署。

对于那些真的很大的神经网络，训练一个完整的组合模型是不可能的（infeasible），但是作者们展示了，和“训练非常长的时间，来训练一个真的很大的单个模型”相比，可以通过训练很多个专家网络，来极大提升性能，其中每一个专家网络会学习去区分一些高度易混淆的类别。作者们还没有做到说可以将这些专家模型中的知识，反向迁移回到那个单独的大网络。

















