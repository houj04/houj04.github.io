---
layout: post
title: "Deep Gradient Compression"
categories: mypost
---

## 论文介绍

* 文章标题：Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training
* 作者：Yujun Lin, Song Han, Huizi Mao, Yu Wang, William J. Dally
* 发表于：ICLR 2018

## 摘要

大规模分布式训练（distributed training）需要非常多的通信带宽来交换梯度，因此限制了多节点训练的扩展性（scalability），并且需要昂贵的高带宽网络基础设施。当需要在移动设备上进行分布式训练的时候，情况会进一步恶化，会遇到更高的延迟，更低的吞吐，以及断断续续的不稳定连接。在这篇文章中，作者们发现，在分布式随机梯度下降（stochastic gradient descent，SGD）中，99.9%的梯度交换是冗余的，进而提出了一个Deep Gradient Compression（DGC）的方法来极大程度上减小通信带宽。为了保证用这种压缩之后的准确度，DGC用了以下四种方案：动量修正、本地梯度截断、动量因子掩码、热启动训练。作者们将DGC方法应用到了图像识别（image classification）、语音识别（speech recognition）以及语言模型中（language modeling），用了不同的数据集，如Cifar10，ImageNet，Penn Treebank以及Librispeech Corpus。在这些场景中，DGC方法在不损失精度的前提下，获得了270x到600x不等的梯度压缩率，降低了梯度的大小：ResNet-50从97MB降低至0.35MB，DeepSpeech从488MB降低至0.74MB。DGC方法使得用不昂贵的1Gbps的以太网络进行大规模分布式训练成为可能，也让移动设备上的分布式训练成为可能。

## 1 介绍

大规模分布式训练提升了训练更深和更大的模型的生产力。同步的随机梯度下降（SGD）在分布式训练中应用广泛。增加更多的训练节点，以及利用数据并行（data parallelism），在同样大小的数据集上的前向后向的整体计算时间可以很大程度上减少。然而，梯度交换的消耗很大，进而阻碍了计算时间的进一步降低，特别是对循环神经网络（recurrent neural networks，RNN），因为它们的计算通信比很低。因此，在提升分布式训练的规模时，网络带宽成为了一个显著的瓶颈。如果分布式训练是在移动设备上进行的话，这个带宽问题会更加明显，例如在联合训练（federated learning）中。在移动设备上训练由于有更好的隐私和个性化，因此有吸引力，但是一个严重的问题是，这些移动设备的网络带宽更低，网络连接的抖动更厉害，并且数据流量也更贵。

DGC用压缩梯度的方法解决了通信带宽问题。为了保证没有精度损失，DGC在梯度离散化之上，还使用了动量修正和本地梯度截断，来保证模型的质量。DGC另外还用了动量因子掩码和热启动训练，来解决通信降低之后的“旧的更新”（staleness）问题。

作者们在一系列任务、模型和数据集上验证了DGC的效果：图像分类的CNN（用了Cifar10和ImageNet），语言模型的RNN（Penn Treebank）、语音识别（Librispeech Corpus）。这些实验显示了梯度可以被压缩到600x，而没有精度损失，这比前人的工作提升了一个数量级。

## 2 相关工作

研究者们已经提出了很多方法，来解决分布式训练中的通信瓶颈问题。举例来说，异步SGD算法，去掉了梯度的同步，当一个节点完成了一次后向传播（back-propagation）之后就会立即更新参数。另外也有广泛的研究，用梯度量化（quantization）和稀疏化（sparsification）来减少通信数据的大小。

梯度量化：将梯度量化到低精度的数值可以减少通信带宽。前人提出过单比特SGD，将梯度传输的数据量降低，获得了传统语音识别应用上面10x的加速比。有人提出QSGD，在准确率和梯度精度上取了平衡。类似QSGD，有人发明了TernGrad，用了三元梯度。这些方法都显示了量化后的训练也是可以收敛的，尽管QSGD只在CNN上进行了实验，而QSGD只在RNN上。另外还有一些对整个模型（包括梯度）进行量化的尝试，例如DoReFa-Net用了1比特的权重和2比特的梯度。

梯度稀疏化：有人提出了阈值量化法，只传输那些比某个预先设定的阈值大的梯度。然而，在实践中很难选择阈值。因此，又有人提出了根据正梯度更新和负梯度更新按比例的方法，后面还有根据绝对值和单个阈值对梯度进行稀疏化的做法（Gradient Dropping）。为了保证收敛速度，GD需要添加对层的归一化。GD节省了99%的梯度交换，在机器翻译任务上带来了0.3%的BLEU指标下降。同时，还有人提出了根据本地梯度活动来动态调整压缩率，获得了对全连接层的200x的压缩比，以及对卷积层的40x的压缩比。并且在ImageNet数据集上的top-1准确率只有微不足道的下降。

和前人的工作相比较，DGC把整个模型的梯度压缩率提升到了600x（对所有层的压缩率相同）。DGC不需要额外的层的归一化，也不需要修改模型结构。更重要的是，DGC的方法的结果上是没有准确度损失的。

## 3 DGC

### 3.1 梯度稀疏化

作者们通过只发送重要的梯度（稀疏更新，sparse update）来降低通信带宽。作者们把梯度的大小（magnitude）当作重要性的简单启发式方法：只有超过某个阈值的梯度才进行传输。为了避免丢失信息，需要将剩余的梯度进行本地累加。最终，这些梯度就变得足够大以致于需要传输了。因此，总是先立即传输较大的梯度，但是最后总会把所有的梯度都发送出去。如原文中的算法1，`encode()`函数把32比特的非0梯度进行打包并且16比特的0的长度（啥意思？）

这个方法的洞察（insight）在于本地梯度的合并，和随时间增加批量数据大小（batch size），是等价的。记需要优化的损失函数是$F(w)$，同步的分布式SGD（Synchronous Distributed SGD）在一共有$N$个节点时，实际执行了下面这样的更新：

$$ F(w) = \dfrac{1}{|X|}\sum_{x \in X} f(x,w), w_{t+1} = w_t - \eta \dfrac{1}{Nb}\sum_{k=1}^n \sum_{x \in B_{k,t}} \nabla f(x, w_t) $$

上式中，$X$是训练集，$w$是网络中的权重，$f(x,w)$是对于$x \in X$样本计算出来的损失，$\eta$是学习率，$N$是训练所用的节点数。$B_{k,t}$对于$1\le k \lt N$是从数据集$X$中随机选出的$N$个mini-batch的一个序列，第$t$次迭代，每个的大小是$b$。

考虑把权重$w$进行扁平化（flatten）之后得到的第$i$个位置上的权重值$w^{(i)}$。在$T$次迭代之后，有：

$$ w_{t+T}^{(i)} = w_t^{(i)} - \eta T \cdot \dfrac{1}{NbT}\sum_{k=1}^N \left(\sum_{\tau=0}^{T-1} \sum_{x \in B_{k, t+\tau}} \nabla^{(i)}f(x,w_{t+\tau}) \right)$$

式2显示了，本地的梯度求和，可以看成是增大了batch size，从$Nb$增加到$NbT$（第二个对$\tau$的求和）。这里的$T$，是两次迭代中间，当$w^{(i)}$被发送出去时候的稀疏更新区间的长度。学习率缩放（learning rate scaling）是一种常被使用的技术，用来应对较大的minibatch。在式2中恰好被满足，因为学习率$\eta T$中的$T$和batch size大小$NbT$中的$T$可以抵消掉。

### 3.2 改进局部梯度累加

如果不小心操作的话，当稀疏程度极高时，稀疏更新对收敛有很大的影响。举例来说，算法1在Cifar10数据集上会导致准确率下降1.0%。作者们找到动量修正方法，以及局部梯度截断，来减轻这个问题。

动量修正：

带动量项的SGD通常会替代普通SGD而被广泛应用。然而，算法1没有直接在SGD中用动量项，因为它忽略了不同稀疏更新区间的衰减因子。

$N$个训练节点，分布式训练，用普通的带动量的SGD，可以写成这样：

$$ u_t = m u_{t-1} + \sum_{k=1}^N \left( \nabla_{k,t} \right), w_{t+1} = w_t - \eta u_t$$

上式中$m$是动量项，$N$是训练节点的数目，并且$\nabla_{k,t} = \frac{1}{Nb}\sum_{x \in B_{k, t}}\nabla f(x, w_t)$。

考虑将权重$w$进行展开之后的第$i$个位置的权重值$w^{(i)}$。在经过$T$次迭代之后，权重值$w^{(i)}$的修改是：

$$ w_{t+T}^{(i)} = w_t^{(i)} - \eta \left[ \cdots + \left(\sum_{\tau=0}^{T-2} m^\tau \right)\nabla_{k, t+1}^{(i)} + \left(\sum_{\tau=0}^{T-1} m^\tau \right)\nabla_{k,t}^{(i)} \right]$$

如果直接把带动量的SGD应用到稀疏梯度的场景（原文中的算法1的第15行），那么更新的方法和公式3就不一样了，而是：

$$ v_{k,t} = v_{k, t-1} + \nabla_{k,t}, u_t = mu_{t-1} + \sum_{k=1}^N sparse(v_{k,t}), w_{t+1} = w_t - \eta u_t$$

上式中的第1项是在训练节点$k$上的局部梯度累加和。当累加结果$v_{k,t}$超过某个阈值的时候，会通过`sparse()`函数中的硬限，在第2项中被编码并且通过网络发出去。类似与算法1的第12行，累加结果$v_{k,t}$被`sparse()`函数中的掩码给清理掉。

权重值$w^{(i)}$的更新，在经过了$T$个稀疏更新之后变成了

$$ w_{t+T}^{(i)} = w_t^{(i)} - \eta \left( \cdots + \nabla_{k, t+1}^{(i)} + \nabla_{k,t}^{(i)} \right) $$

在公式6中，累加的衰减因子$\sum_{\tau=0}^{T-1} m^\tau$的消失，和公式4比，导致了收敛性能的损失。在原文中图2a中可以看到，公式4将优化方向从点A指向点B，但是用了局部梯度累加之后，公式4会走到点C去。如果梯度稀疏性很高，更新区间$T$大幅上升，进而这个显著的副作用会伤害到模型的质量。为了避免这种错误，需要在公式5中进行动量的修正，保证稀疏更新和公式3中的稠密更新一致。

如果把公式3中的速度$u_t$看成是“梯度”，那么公式3中的第二项可以看出是对“梯度”$u_t$的一个普通SGD更新。在节3.1中已经说明了局部梯度累加对普通SGD的有效性，因此可以在局部对速度$u_t$进行累加，替换掉真正的梯度$\nabla_{k,t}$，这样可以让公式5接近公式3：

$$ u_{k,t} = mu_{k, t-1} + \nabla_{k,t}, v_{k,t} = v_{k,t-1} + u_{k,t}, w_{t+1} = w_t - \eta \sum_{k=1}^N sparse(v_{k,t})$$

上式中，前两项是修正之后的局部梯度累加，累加结果$v_{k,t}$会用来做后续的稀疏化和通信。在局部累加中用了这个简单的修改之后，就可以推断出公式4中的累加的衰减系数$\sum_{\tau=0}^{T-1} m^\tau$，如原文中图2b所示。

作者们把这种“迁移”叫做动量修正。这是对更新公式的一种调整，并不引入任何的超参数（hyper parameter）。除了普通的带动量的SGD，作者们也研究了相似的带动量的Nesterov SGD，在文章的附录B里。

局部梯度截断：

梯度截断是一种常用的解决梯度爆炸（exploding gradient）问题的方法。后续又有人提出过一些改进的方法，例如当梯度的L2范数（L2-norm）之和超过某个阈值的时候重新对梯度进行缩放。这个步骤通常是在从所有节点执行梯度聚合之后才执行的。由于本文作者提出的方法是在各个节点上独立进行梯度累加，因此需要在把当前梯度$G_t$加到以前的累加和（即，文中算法1的$G_{t-1}$）之前，来执行梯度截断。

文章的附录C中有解释，作者们把阈值缩放到$N^{-1/2}$。在实践中，作者们发现局部梯度截断的行为，和普通的训练中对梯度进行截断类似，这说明作者们提出的假设在真实数据上可能是有效的。

在后面的实验阶段（即节4）中可以看到，动量修正和局部梯度截断可以把AN4数据集的词错误率从14.1%降低到12.9%，而训练曲线和动量SGD更接近。

### 3.3 解决更新迟缓的问题

由于对于梯度的小更新延迟了，那么当这些更新出现的时候，其实已经过时（stale）。在本文作者的实验中，当梯度的稀疏程度是99.9%的时候，，大多数的参数会每600到1000个迭代（iteration）才会更新一次。和每轮（epoch）有多少个迭代相比，这个数太大了。更新的滞后性会导致收敛速度的降低，并且会使得模型质量变差。作者们用了下面的两个方法来解决更新延迟的问题：

动量因子掩码：前人的工作讨论了由于异步更新而导致的更新缓慢的问题，并且定义了一个叫做“隐动量”的项。受到他们工作的启发，作者们提出了动量因子掩码，来解决更新滞后的问题。和前人的工作不同的是，没有搜索一个新的动量系数，而是简单地把同样的掩码应用到公式7中的累加的梯度$v_{k,t}$和动量因子$u_{k,t}$：

$$ Mask \leftarrow |v_{k,t}| > thr, v_{k,t} \leftarrow v_{k,t} \odot \neg Mask, u_{k,t} \leftarrow u_{k,t} \odot \neg Mask $$

这个掩码（mask）会把延后的梯度的动量停止住，阻止延后的动量把权重带向错误的方向。

热启动训练：
在刚开始训练的时候，网络会改变很快，计算出来的梯度范围很大，值也很大。对梯度进行稀疏化限制了模型的范围，进而延长了网络快速改变的时间长度。同时，从早期阶段积攒下来的值很大的梯度，在被选择到下一步更新之前一直累加，因此可能比最新的梯度还要重要然后就会误导优化方向。

在大minibatch训练中引入的热启动方法在这件事情上是有帮助的。在热启动阶段，首先用一个比较小的学习率，来降低网络的变化速率，同时用一个比较小的梯度稀疏程度，来减少被延迟的极端梯度的数量。和一般的方法，在前几轮中线性提升学习率的做法不同，作者们指数提升梯度稀疏度，从一个相对较小的值升到最终值，用来帮助调整更大稀疏性的梯度。

从表格1中可以看出，动量修正和局部梯度截断，可以提升局部梯度的求和。而梯度因子掩码和热启动训练减轻了梯度过时的效应。在梯度稀疏化和局部梯度累加之上，这4个技术构成了DGC（伪代码参见原文的附录D），帮助将梯度的压缩率提升到更高，同时保证了模型的准确率。

